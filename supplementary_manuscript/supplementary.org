#+TITLE: Supplementary Material \\ \small{Firing rate response of neocortical pyramidal neurons in the fluctuation-driven regime}
#+AUTHOR: Y. Zerlaut, B. Telenczuk,  C. Deleuze, T. Bal, G. Ouanounou* \& A. Destexhe*

\newpage

* Details about the experimental data
:PROPERTIES:
:CUSTOM_ID: exp-details
:END:


 For each cell, its properties and the quality of the electrical
 access was quantified. We present here those data and look for
 relations between them.

# --> FIGURE <-- #
#+BEGIN_LATEX
\begin{figure}[htb!]
\centering
\includegraphics[width=.6\linewidth]{figures/fig_experimental_details}
\caption{\bfseries Details about the presented dataset (animal age,
  electrical access and membrane properties).
\normalfont 
\textbf{(A)} Histogram of the access resistance.
\textbf{(B)} Histogram of the "Seal Quality", the current leak between
the pipette and the patch of membrane.
\textbf{(C)} Histogram of the full recording time. Corresponding either to
the loss of cellular access (rarely) or to the exit of the criteria
formulated in Section \ref{monitoring} (most common case).
\textbf{(D)} Histogram of the membrane time constants.
\textbf{(E)} Histogram of the membrane input resistance.
\textbf{(F)} Histogram of the animal post-natal day per 
recorded cell.
\textbf{(G)} Cross correlations (Pearson correrlation) between
all monitored quantities.}
\label{fig-experimental-details}
\end{figure}
#+END_LATEX

Some of the relations that appear are :

- Very naturally, the membrane time constant is proportional to the input resistance
  (c=0.8, $p<2.10^{-10}$)

- The recording time diminishes with the age of the animal 
  ($p<1.10^{-3}$)

- The membrane resistances and membrane time constants decrease with
  the age of the animal even if they keep a strong variability
  ($p<5.10^{-3}$ and $p<3.10^{-2}$ respectively).

- The access resistant seems independent of all parameters

- The quality of the seal does not seem to impede much the duration of
  the recording (despite the fact that they are both correlated with
  the age of the animal, there mutual correlation is low).

\newpage

*** histograms 							   :noexport:

#+begin_src python
import sys
sys.path.append('/home/yann/work/python_library/')
from my_graph import set_plot

import matplotlib.pylab as plt
import numpy as np
sys.path.append('../experimental_data/')
from dataset_structure import load_params_of_dataset

CELL_INDEX, RS, ILEAK, RM, CM, TM, RECORDING_LENGTH, PST_NATAL,\
   N_SPIKES, DURATION = np.load('../experimental_data/dataset.npy')

INTEREST = [RS, ILEAK, RM, TM, RECORDING_LENGTH, PST_NATAL]
INTEREST_LABEL = [r'$R_\mathrm{S}$ (M$\Omega$)',\
    r'$I_\mathrm{leak}$ (pA)', '$R_\mathrm{m}$ (M$\Omega$)',\
    r'$\tau_\mathrm{m}^0$ (ms)', '$T_\mathrm{rec}$ (min)',\
    'P (day)']
INTEREST_LABEL2 = ['Access Resistance', 'Seal Quality',\
    'Membrane Resistance', 'Membrane Time Constant',\
    'Full Recording Time', 'Post-Natal Day']
LABEL = ['RS', 'ILEAK', 'RM', 'TM', 'RECORDING_LENGTH', 'PST_NATAL']

for i in range(len(INTEREST)):
   fig, ax = plt.subplots(1, 1, figsize=(4,3))
   plt.subplots_adjust(bottom=.4, left=.3)
   ax.hist(INTEREST[i], bins=15, color='grey', lw=2)
   set_plot(ax, ylabel='cell count', xlabel=INTEREST_LABEL[i])
   ax.annotate(INTEREST_LABEL2[i],\
        (0.3,0.05), xycoords='figure fraction', fontsize=17)
   fig.savefig('../figures/'+LABEL[i]+'.svg', format='svg', transparent=True)

plt.show()
#+end_src

#+RESULTS:


*** correlations 						   :noexport:

#+begin_src python
import sys
sys.path.append('/home/yann/work/python_library/')
from my_graph import set_plot

import matplotlib.pylab as plt
import numpy as np
from scipy.stats.stats import pearsonr
sys.path.append('../experimental_data/')
from dataset_structure import load_params_of_dataset

CELL_INDEX, RS, ILEAK, RM, CM, TM, RECORDING_LENGTH, PST_NATAL,\
   N_SPIKES, DURATION = np.load('../experimental_data/dataset.npy')

INTEREST = [RS, ILEAK, RM, TM, RECORDING_LENGTH, PST_NATAL]
INTEREST_LABEL = [r'$R_\mathrm{S}$ (M$\Omega$)',\
    r'$I_\mathrm{leak}$ (pA)', '$R_\mathrm{m}$ (M$\Omega$)',\
    r'$\tau_\mathrm{m}^0$ (ms)', '$T_\mathrm{rec}$ (min)',\
    'post-natal day']

fig, AX = plt.subplots(len(INTEREST), len(INTEREST)-1, figsize=(15,20))
plt.subplots_adjust(wspace=.3, hspace=.5)
AX.reshape((len(INTEREST), len(INTEREST)-1))
for i in range(len(INTEREST)):
   for j in range(i):
     x = INTEREST[j]
     xth = np.linspace(x.min(), x.max())
     y = INTEREST[i]
     AX[i,j].plot(x, y, 'kD', ms=5)
     cc, pp = pearsonr(x, y)
     AX[i,j].plot(xth, np.polyval(np.polyfit(x, y, 1), xth), 'k-')
     AX[i,j].annotate('c='+str(np.round(cc,1))+', p='+'%.1e' % pp,\
                    (0.1,1.03), xycoords='axes fraction', fontsize=14)
     if j==0:
        ylabel=INTEREST_LABEL[i]
     else:
        ylabel=''
     if i==len(INTEREST)-1:
        xlabel=INTEREST_LABEL[j]
     else:
        xlabel=''
     set_plot(AX[i,j], xlabel=xlabel, ylabel=ylabel)

for i in range(len(INTEREST)):
   for j in range(i, len(INTEREST)-1):
     AX[i,j].axis('off')

fig.savefig('figures/fig_experimental_correlations.svg', format='svg', transparent=True)

plt.show()
#+end_src

#+RESULTS:
: None


*** multi-panel :noexport:

#+begin_src python
import svgutils.transform as sg
fig = sg.SVGFigure("8.5cm", "10.5cm")
fig1 = sg.fromfile('../figures/experimental_correlations.svg')
fig2 = sg.fromfile('../figures/RS.svg')
fig3 = sg.fromfile('../figures/ILEAK.svg')
fig4 = sg.fromfile('../figures/RECORDING_LENGTH.svg')
fig6 = sg.fromfile('../figures/RM.svg')
fig5 = sg.fromfile('../figures/TM.svg')
fig7 = sg.fromfile('../figures/PST_NATAL.svg')

txt1 = sg.TextElement(0,50, "G", size=11, weight='bold')
txt2 = sg.TextElement(70,10, "A", size=11, weight='bold')
txt3 = sg.TextElement(140,10, "B", size=11, weight='bold')
txt4 = sg.TextElement(210,10, "C", size=11, weight='bold')
txt5 = sg.TextElement(140,83, "D", size=11, weight='bold')
txt6 = sg.TextElement(210,83, "E", size=11, weight='bold')
txt7 = sg.TextElement(210,157, "F", size=11, weight='bold')

# add text labels

# append plots and labels to figure
plot1 = fig1.getroot();plot1.moveto(-15, -60, scale=.315)
plot2 = fig2.getroot();plot2.moveto(60, 10, scale=.3)
plot3 = fig3.getroot();plot3.moveto(130, 10, scale=.3)
plot4 = fig4.getroot();plot4.moveto(200, 10, scale=.3)
plot5 = fig5.getroot();plot5.moveto(130, 80, scale=.3)
plot6 = fig6.getroot();plot6.moveto(200, 80, scale=.3)
plot7 = fig7.getroot();plot7.moveto(200, 150, scale=.3)
fig.append([plot4, plot3, plot2, plot6, plot5, plot7, plot1])
fig.append([txt1, txt2, txt3, txt4, txt5, txt6, txt7])

# save generated SVG files
fig.save("../figures/experimental_details.svg")

import os
os.system('inkscape --export-pdf=../figures/experimental_details.pdf ../figures/experimental_details.svg')
os.system('eog ../figures/experimental_details.svg')

#+end_src

#+RESULTS:
: None


* Accuracy of the single compartment approximation
:PROPERTIES:
:CUSTOM_ID: single-comp-data
:END:

The single-compartment approximation is important in this study as it
is used to constrain the fluctuations of the membrane potential. We do
here a cell by cell quantification of the accuracy of the
approximation. 

 We quantify the accuracy of the approximation as follows. We take the
 protocols that were used to determine the membrane properties: prior
 to each protocol, we recorded and averaged the response to 10 current
 pulses of $\sim$ 500ms and of $\Delta I \sim$ 15pA amplitude, not the
 (noisy) continuous monitoring presented in Figure [[fig-exp-charact]]. We
 average over trials the membrane potential response and fit an
 exponential load to this mean response
 $V_\mathrm{sc}^\mathrm{fit}(t)$, we get a membrane time
 $\tau_\mathrm{m}^0$ and a membrane resistance $R_\mathrm{m}^0$. We
 define the residual trace as the normalized absolute difference
 between the fit and the trace :

\begin{equation}
\mathrm{Res}(t) = \frac{ \| V(t) - V_\mathrm{sc}^\mathrm{fit}(t) \| }{R_m^0 \Delta I}
\label{eq:residual-single-comp}
\end{equation}

 
 Now we quantify the accuracy of the single-compartment approximation
 by taking the (normalized) integral over 7 membrane time constant of
 the residual trace (see bottom traces in [[fig-single-comp-approx]]B and
 [[fig-single-comp-approx]]C).

\begin{equation}
C_\mathrm{sc} = \int_{t_0}^{t_0+7 \, \tau_m^0} \frac{dt}{7\,\tau_m^0}
\mathrm{Res}(t)
\label{eq:accuracy-coeff-single-comp}
\end{equation}

 The two normalizations (by membrane resistance and membrane time
 constant) were performed to yield comparable quantities for different
 membrane parameters.

 We present the histogram of this quantity over the
 cells of the dataset in Figure [[fig-single-comp-approx]]A.

# --> FIGURE <-- #
#+BEGIN_LATEX
\begin{figure}[htb!]
\centering
\includegraphics[width=.99\linewidth]{../figures/single_comp.pdf}
\caption{\bfseries Accuracy of the single compartment approximation in the neocortical
neurons of our recordings.
\normalfont \textbf{(A)} Histogram of the accuracy coefficient $C_\textrm{sc}$.
\textbf{(B)} Neuron showing the best accuracy coefficient.
\textbf{(C)} Neuron showing the worst accuracy coefficient.}
\label{fig-single-comp-approx}
\end{figure}
#+END_LATEX

 We conclude that the approximation was satisfactory : the worst case
in Figure [[fig-single-comp-approx]]C corresponds to a pretty good
match. 

\clearpage 

**** analysis 							   :noexport:

#+begin_src python
import sys
sys.path.append('/home/yann/work/python_library/')
from electrophy import IC_membrane_test as IC
sys.path.append('../experimental_data/')
import dataset_structure as DATA
import numpy as np

RESIDUAL_LIST, CELL_LIST = [], []

Tm_factor = 6

for i in DATA.CELL_LIST[np.concatenate([np.arange(28), np.arange(29,len(DATA.CELL_LIST))])]:
   exec('from cell'+str(i)+' import cell'+str(i))
   exec('cell = cell'+str(i)+'.cell_params')
   exp, time, t, data, params = IC.load(cell['ROOT_FOLDER']+cell['IC_datafile'])
   exp, time, t, data, params, Rm, El, Cm, t_fit, v_fit,\
       RmS, CmS, Ra, RmD, CmD, v_fit_2comp, mean_v_response, mean_i = \
                      IC.analyze(exp, time, t, data, params)
   Tm = Rm*Cm*1e-3
   dt = t[1]-t[0]
   DI = np.abs(np.diff(mean_i[5:])).max() # pA, pulse
   # we find where the pulse start !
   i1 = np.where(np.abs(np.diff(mean_i[5:]))>.6*DI)[0]
   it = np.arange(i1[0], min([i1[0]+int(Tm_factor*Tm/dt), len(t_fit)-1]))
   residual = np.abs((mean_v_response[it]-v_fit[it])/(v_fit[-1]-v_fit[0]))
   RESIDUAL_LIST.append(residual.sum()*dt/Tm/Tm_factor)
   CELL_LIST.append(i)

# then the cell28, that has an AP in one trial and can not be evaluated !
CELL_LIST.append(DATA.CELL_LIST[28])
RESIDUAL_LIST.append(np.array(RESIDUAL_LIST).mean())
CELL_LIST = np.array(CELL_LIST)
RESIDUAL_LIST = np.array(RESIDUAL_LIST)
isort = np.argsort(CELL_LIST)
np.save('../experimental_data/analyzed_data/residuals.npy',\
                   [CELL_LIST[isort], RESIDUAL_LIST[isort]])
#+end_src

#+RESULTS:
: None


**** plot 							   :noexport:

#+begin_src python
import sys
sys.path.append('/home/yann/work/python_library/')
from electrophy import IC_membrane_test as IC
from my_graph import set_plot
sys.path.append('../experimental_data/')
import dataset_structure as DATA
import matplotlib.pylab as plt
import numpy as np

CELL_LIST, RESIDUAL_LIST = \
     np.load('../experimental_data/analyzed_data/residuals.npy')


figA = plt.figure(figsize=(4,3))
plt.subplots_adjust(bottom=.25, left=.25)
ax = plt.subplot(111)
plt.hist(RESIDUAL_LIST, bins=15, color='grey')
set_plot(ax, xlabel='$C_{sc}$', ylabel='cell count')

Tm_factor = 10

imax = np.argmax(RESIDUAL_LIST)
imin = np.argmin(RESIDUAL_LIST)

FIG = []
for i in [DATA.CELL_LIST[imin], DATA.CELL_LIST[imax]]:
   f, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(5,5))
   plt.subplots_adjust(bottom=.15, left=.25)
   FIG.append(f)
   exec('from cell'+str(i)+' import cell'+str(i))
   exec('cell = cell'+str(i)+'.cell_params')
   exp, time, t, data, params = IC.load(cell['ROOT_FOLDER']+cell['IC_datafile'])
   exp, time, t, data, params, Rm, El, Cm, t_fit, v_fit,\
       RmS, CmS, Ra, RmD, CmD, v_fit_2comp, mean_v_response, mean_i = \
                      IC.analyze(exp, time, t, data, params)
   Tm = Rm*Cm*1e-3
   dt = t[1]-t[0]
   DI = np.abs(np.diff(mean_i[5:])).max() # pA, pulse
   # we find where the pulse start !
   i1 = np.where(np.abs(np.diff(mean_i[5:]))>.6*DI)[0]
   it = np.arange(i1[0], min([i1[0]+int(Tm_factor*Tm/dt), len(t_fit)-1]))
   residual = np.abs((mean_v_response[it]-v_fit[it])/(v_fit[-1]-v_fit[0]))
   ax1.plot(t_fit, mean_i[:len(t_fit)], 'k')
   set_plot(ax1, ylabel='I (pA)', spines=['left'])
   ax2.plot(t_fit, mean_v_response[:len(t_fit)], 'k')
   ax2.plot(t_fit[it], v_fit[it], 'r--', lw=3)
   set_plot(ax2, ylabel='$V_m$ (mV)', spines=['left'])
   ax3.plot(t_fit[it], residual, 'k-')
   ax3.plot(t_fit, 0*t_fit, 'k-')
   ax3.fill_between(t_fit[it], residual, 0*residual, color='grey')
   ax3.plot([10,10],[0,0.08],color='w')
   set_plot(ax3, ylabel='residual', xlabel='time (ms)')

figA.savefig('../figures/single_comp_accuracy_hist.svg', format='svg', tranparent=True)
FIG[0].suptitle('best match')
FIG[0].savefig('../figures/single_comp_accuracy_best.svg', format='svg', tranparent=True)
FIG[1].suptitle('worst match')
FIG[1].savefig('../figures/single_comp_accuracy_worst.svg', format='svg', tranparent=True)

import svgutils.transform as sg
fig = sg.SVGFigure("12.5cm", "4.5cm")

# load matpotlib-generated figures
fig1 = sg.fromfile('../figures/single_comp_accuracy_hist.svg')
fig2 = sg.fromfile('../figures/single_comp_accuracy_best.svg')
fig3 = sg.fromfile('../figures/single_comp_accuracy_worst.svg')

# get the plot objects
plot1 = fig1.getroot();plot1.moveto(2, 20, scale=.5)
plot2 = fig2.getroot();plot2.moveto(160, 2, scale=.4)
plot3 = fig3.getroot();plot3.moveto(310, 2, scale=.4)

# add text labels
txt1 = sg.TextElement(0,20, "A", size=14, weight="bold")
txt2 = sg.TextElement(155,15, "B", size=14, weight="bold")
txt3 = sg.TextElement(305,15, "C", size=14, weight="bold")

# append plots and labels to figure
fig.append([plot1, plot2, plot3])
fig.append([txt1, txt2, txt3])

# save generated SVG files
fig.save("../figures/single_comp.svg")

import os
os.system('inkscape --export-pdf=../figures/single_comp.pdf ../figures/single_comp.svg')
os.system('eog ../figures/single_comp.svg')
# os.system('rm fig2.svg')

#+end_src

#+RESULTS:
: None


\newpage


* Finite sampling of a Poisson process
:PROPERTIES:
:CUSTOM_ID: finite-sampling
:END:

Three main components could be identified as contributing to the
measured dispersion of the firing rate dependencies. 1) cellular
heterogeneity, 2) experimental changes across experiments and 3)
finite sampling of the irregular firing process.

Because the cellular heterogeneity point is the biologically relevant
phenomena [[cite:Mejias2012]] that we would like to evaluate, we try
here to estimate the contribution of the finite sampling effect so
that we get an higher bound for the cellular heterogeneity (higher
bound because contribution of the experimental bias is unknown).

Experimentally, we estimate the firing rate on a finite amount of time
$T$. Because the firing process is irregular, this will induce a
dispersion around the mean cellular behavior. 

The Poisson process has been shown to be a good model for the
irregularity of the firing process *REF?*, so that this calculation
will take this as an assumption. Given an ideal frequency $\nu$
evaluated over a time $T$, the probability to observe a frequency
$\nu_\mathrm{obs} = k/T$ ($k$ is the number of observed spikes) is:

\begin{equation}
  P_{\nu, T}(\nu_{obs} = \frac{k}{T}) = \frac{e^{-\nu \, T}}{T} \cdot
  \frac{(\nu \, T)^{k}}{k!}
\label{eq-poisson-proba}
\end{equation}

Where the mean and standard deviation of observed spike number are
given by: \( \langle k \rangle = \nu_\mathrm{id} T \) and 
\( \sqrt{\langle \big( k^2 - \langle k \rangle^2 \big) \rangle}
 = \sqrt{\nu_{id} \, T} \).

Let's say that we study the firing rate as a function of a variable
$x$ (e.g. $\tau_V/\tau_\mathrm{m}^0$). We scan $N$ points of this
variable $x$ (the $x_i$ where $i \in [1,N]$) that we each repeat $S$
times by varying the seed (indexed by $s \in S$). One trial result in
a spike number $k_i^s$, therefore the whole experiment results in the
set $\{ k_i^s \}$. Now we assume, that the process has a well defined
dependency on $x$ (e.g. as given by \ref{eq-Tv-shift} for $\tau_V /
\tau_\mathrm{m}^0$) so that the Poisson process has the frequency
$\nu(x_i)$ for the trials scanning $x_i$. Then probability to observe
the set $\{ k_i^s \}$ given a finite sampling of length $T$ (assuming
independence between experiments) is:

\begin{equation}
P( \{ k_i^s \}) = \mathrm{e}^{-S \, T \, \sum_{i} \nu(x_i)} \times
\Pi_i \frac{(\nu(x_i) \, T)^{\sum_s k_i^s}}{\Pi_s \, k_i^s!}
\label{eq-poisson-set-proba}
\end{equation}

We evaluated the response heterogeneity on the coefficients of the
\textit{effective threshold} so we need to translate the set of
measurements $\{ k_i^s \}$ into a set of firing rate $\{ \nu_i^s =
k_i^s/T \}$ and then into a coefficient for the \textit{effective
threshold} (e.g. $\Delta_{\tau_v}$ for $\tau_v /
\tau_\mathrm{m}^0$). So each possible measurement $\{ k_i^s \}$ is
converted into a coefficient with its probability $P( \{ k_i^s
\})$. We should then test for all possible measurements $\{ k_i^s \}$,
but in practice (because it is useless to span the whole space of
possibilities), for each point, we consider values of observed spikes
delimited by three standard deviations around the mean number of
possible spike.

So, for each type of protocol, we take the average behavior (in terms
of /phenomenological threshold/), we convert it to a firing rate
thanks to the average ($\mu_V$, $\sigma_V$, $\tau_V$), we take the
average recording conditions (number of points and seeds) and we
evaluate the variations expected from those conditions (the procedure
is illustrated in Figure [[finite-sampling-poisson]]). Because of the
multiple averaging (and the fact that the expected variations are non
linearly related to the /effective threshold/), the result is not
exactly what would be expected from a Poisson process having this
dependency but this provide a reasonable first guess.


#+BEGIN_LATEX
\begin{figure}[h!]
  \centering
  \includegraphics[width=.35\linewidth]{../figures/finite_sampling_Fobs}
  \includegraphics[width=.35\linewidth]{../figures/finite_sampling_k_possible} \\
  \includegraphics[width=.4\linewidth]{../figures/finite_sampling_k_observed}
  \includegraphics[width=.35\linewidth]{../figures/finite_sampling_hist}
  \caption{\textbf{Quantifying the dispersion due to the 
      sampling over a finite time of the irregular spiking process.
     Insight from the case of a Poisson process}.\\
    \textbf{(A)} Let's say that the neuron has a real physical
    dependency of its threshold to a variable $x$ (here a linear
    dependency $V_\mathrm{thre}^\mathrm{eff}= -50$mV + $(x-1) \cdot 1$
    mV). \textbf{(B)} The measurement protocol is made of varies 3
    times the variable $x$, for 2 different seeds and for a recording
    time of $T=$3s. We consider the possible spikes up to 3 standard
    deviations around the most probable observed spike
    number. \textbf{(C)} Four examples of observations (translated
    into the phenomenological threshold anhd with the resulting linear
    fit) with their respective probability (normalized to
    maximum). \textbf{(D)} Expected dispersion as a consequence of the
    finite sampling (evaluated over 50$^3$ observations)}
  \label{fig:finite-sampling-poisson}
\end{figure}
#+END_LATEX

\clearpage


* Accuracy of the template : insight from an analytically solvable situation :noexport:
:PROPERTIES:
:CUSTOM_ID: brunel-comp
:END:

The situation of an Integrate and Fire neuron stimulated by
delta-synapses can be instructive because we benefit from a very
accurate analytical approximation in our regime of interest
[[cite:Amit1997]]. We will use this model to investigate the properties
and limitations of the \textit{crude approximation} for the firing
rate. \\

We stimulate a neuron with a Poisson process of frequency
$\nu_\mathrm{in}$, where each event triggers a jump of membrane
potential $J$ followed by a decay of time constant $\tau$. At
relatively high frequencies ($\nu_\mathrm{in} \gg
\tau_\mathrm{m}^{-1}$), we get a fluctuating membrane potential of
mean $\mu_V = E_\mathrm{L} +J\,\tau_\mathrm{m}\,\nu_{in}$, standard
deviation $\sigma_V = J \sqrt{\nu_\mathrm{in} \tau /2}$ and autocorrelation
time $\tau_\mathrm{m}$.

After the so-called /diffusion approximation/, we can transform the
current input made of $\delta$ distribution into a white noise
input. Then the problem corresponds to a Langevin equation with
bounded between $V=-\infty$ and the threshold potential
$V_\mathrm{thre}$. In the stationary case, solving the Fokker-Planck
(FP) equation with the appropriate conditions\footnote{The integrate
and fire mechanism corresponds to \textbf{a}) vanishing probabilities
at the boundaries (Pr$(-\infty)=$Pr$(V_\mathrm{thre})$=0) \textbf{b})
jump of the probability flux at $V_{reset}$ \textbf{c}) probability
continuity and \textbf{d}) probability normalization. See
[[cite:Renart2004]] for details} yield the stationary distribution of the
membrane potential $P(V)$ and the firing rate $\nu_\mathrm{out}$.

\begin{equation}
  \hspace{-.4cm}
  \label{eq-cds-iaf-final}
  \left \lbrace
  \begin{split}
  & \gamma(V) \, \longrightarrow \,  \frac{V-E_\mathrm{L} - \mu_V}{\sqrt{2}
    \, \sigma_V } \\[.3cm]
  & \nu_\mathrm{out} =  \Big( \tau \, \sqrt{\pi} \, 
  \int_{\gamma(V_{reset})}^{\gamma(V_\mathrm{thre})}
  e^{x^2}
  \, \, (\textnormal{Erf}(x)+1) \, \, dx\Big)^{-1} \\[.3cm]
  &
  P(V) =  
  \frac{\sqrt{2} \, \, \nu_\mathrm{out} \, e^{- \big( \gamma(V) \big) ^2}}{\sigma_V}
  \int_{
    \gamma \big(\max (V, V_{reset}) \big)
  }^{\gamma(V_\mathrm{thre})} e^{x^2} \, dx
  \end{split} \right.
\end{equation}

#+BEGIN_LATEX
\begin{figure}[htb!]
\centering
  % \subfloat[][]{
  %   \includegraphics[width=.85\linewidth]{../figures/membrane_potential_proba}
  %   \label{subfig:membrane-pot-distrib}
  % }  \\
  % \subfloat[][]{
  %   \hspace{-.7cm}
  %   \includegraphics[width=.95\linewidth]{../figures/firing_rate_comp_brunel}
  %   \label{subfig:firing-traces}
  % }
  \caption{\textbf{Origin of the deviation between the heuristic
      approximation and the real solution}. Comparing the Fokker
    Planck (FP) solution, the heuristic approximation and numerical
    simulations (see discussion in
    \ref{sec:brunel-comp}). \textbf{(A)} Membrane potential
    distributions in the case of the FP solution
    (\ref{eq-cds-iaf-final}) to the IaF problem (plain line) or
    the simple gaussian approximation (dashed line) \textbf{(B)}
    Associated estimations for the firing rate as a function of the
    input. The stars corresponds to the points of the upper
    figures. The parameters were $V_\mathrm{thre}$=-50mV,
    $V_{reset}$=$E_\mathrm{L}$=-60mV, $\tau_V$=10ms and $J$=0.5mV. Numerical
    simulations corresponds to an \textit{event-based} strategy, note
    the very good match with the FP solution in this parameter range
    [[cite:Brunel1998a]].}
  \label{fig:fpt-wn-insight1}
\end{figure}
#+END_LATEX


In \ref{fig:fpt-wn-insight1}, we examine the difference between
this analytical solution and our naive gaussian approximation. We
stimulate the neuron with presynaptic spike trains of increasing
frequency $\nu_{in}$ (for given $\tau_V$ and $J$) to compare the
estimates of the firing frequency $\nu_\mathrm{out}$ with the numerical
realisation.
\quad \\

# % Of course a
# % fundamenal difference is the fact that the free gaussian is not
# % bounded, but we are interested in the firing probability and this

#+BEGIN_LATEX
\begin{figure}[htb!]
\centering
  %  \includegraphics[width=1\linewidth]{../figures/fpt_wn_insight_varying_\mathrm{m}ean_var}
  %  \includegraphics[width=1\linewidth]{../figures/fpt_wn_insight_varying_\mathrm{m}ean_tau}
  %  \includegraphics[width=1\linewidth]{../figures/fpt_wn_insight_varying_tau_var}
  \caption{\textbf{Varying the parameters of the membrane potential
      fluctuations to study the impact on the defined
      \textit{heuristic threshold}}. We fix one variable in the FP
    solution, $\tau_V$ in \textbf{(A)}, $\sigma_V$ in \textbf{(B)},
    $\mu_V$ in \textbf{(C)} and we make vary the two others.The plots
    correspond to surfaces in a 3D space, we projected those surfaces
    on each of the plane of the coordinate system, in this projection,
    the color (from cold to warm) codes increasing values of the
    remaining coordinate within the surface.}
  \label{fig:fpt-wn-insight2}
\end{figure}
#+END_LATEX



We observe that for low input (\ref{subfig:firing-traces}), the
approximation underestimates the firing probability. This is a
consequence of the boundary conditions. In absence of such conditions,
the solution for the membrane potential distribution would be our
gaussian approximation (dashed lines in
\ref{subfig:membrane-pot-distrib}). But the IaF model imposes
$P(V\geq V_\mathrm{thre})=0$, so this probability content is reinjected below
at $V_{reset}$ and spreads according to a diffusion process, but again
the content (from this additional content) that should go above
$V_\mathrm{thre}$ is reinjected at $V_\mathrm{thre}$, etc... This (small) cumulative
effect brings the unbounded gaussian to underestimate the probability
to be above threshold.

The previous explanation does not consider any temporal
dynamics[fn::even if this is a stationary solution, the fact to have a
non zero probability flux gives a meaning to the temporal dynamics at
the population level], when the firing rate will become high enough,
the mean time to drift toward the threshold will become a limiting
factor for the firing probability. This is what happens at high input,
much of the probability content is made of elements transiting between
thre reset and the threshold (see in
\ref{subfig:membrane-pot-distrib}, the distribution becomes much
thicker at intermediate values). This limiting phenomena is obviously
not taken into account by gaussian approximation, so for this range of
input, the heuristic estimation gives an overestimation of the firing
probability.


Therefore, very grossly, the value where the approximation works (in
\ref{subfig:firing-traces}, where the Fokker Planck solution
crosses the heuristic approximation) corresponds to the point at which
the bias due to the boundary conditions is compensated by the temporal
inertia of the membrane distribution to go toward the threshold.


So depending on where we lie with respect to this point, we will need
to lower or raise the threshold to make the heuristic template
correspond.\\

To investigate more quantitatively how the defined /effective
  threshold/ $V_\mathrm{thre}^\mathrm{eff}$ depends on the parameters
  of the membrane potential fluctuations $\mu_V$, $\sigma_V$ and
  $\tau_V$, in \ref{fig:fpt-wn-insight2} we make vary those
  parameters and study the effect on the firing rate and on the
  associated $V_\mathrm{thre}^\mathrm{eff}$ (It is calculated using
  the inversion formula \ref{eq-effective-threshold}).

On \ref{subfig:fpt-wn-varying-mean-var}, we fix the membrane time
constant and vary $\mu_V$ and $\sigma_V$, for the values of the firing
rate that we are interested in ($\lesssim$ 100Hz) we observe that we
remain in the range where the approximation overestimate the firing
rate, because the \textit{heuristic threshold} needs to be lower than
$V_\mathrm{thre}=-50$mV. This function can be easily fitted by a 2
dimensional second order polynom.

# % On \ref{subfig:fpt-wn-varying-mean-tau} and
# % \ref{subfig:fpt-wn-varying-var-tau} (right figures) we observe an
# % important feature of the approximation, $V_\mathrm{thre}^\mathrm{eff}$ does not seem
# % to depend on $\tau_V$, this means that the template
# % \ref{eq-template} takes very well into account the impact of the
# % temporal dynamics of the membrane potential fluctuations. In addition
# % to the rather simple dependency to $\mu_V$ and $\sigma_V$, this is
# % what makes this template pretty compelling ! \textbf{trivial !! the
# %   analytical formula is 1 over tau, to be removed}

# % \section{The \textit{global autocorrelation time} controls spike
# %   probability}
# % \label{sec:acf-controls-spiking}


* Dependency on the first two moments of the membrane potential fluctuations 
:PROPERTIES:
:CUSTOM_ID: muV-sV
:END:

# --> FIGURE <-- #
#+BEGIN_LATEX
\begin{figure*}[htb!]
\includegraphics[width=.9\linewidth]{fig2.pdf}
\caption{
\bfseries Adapting a simple approximation to construct an analytical
 template for the firing rate response of theoretical models and
 neocortical neurons. \normalfont  \textbf{(A)} Firing rate response of
 the IaF neuron in the  ($\mu_V, \sigma_V$) plane, the other variables
 were set to $\tau_V / \tau_\mathrm{m}^0 =45 \%$ and
 $\mu_G / g_\mathrm{L} = 4$  (see real units in \textbf{B}).  
Numerical data (points with errorbars over trials,
 see Methods in \ref{numerical-tools}) and fitted template.
 A first order polynomial of ($\mu_V, \sigma_V$) was fitted
 for $V_\mathrm{thre}^\mathrm{eff}$ (see inset plot) so that when
 plugged into  Equation \ref{eq-template} it captures the firing rate
 response (large plain line). \textbf{(B)} We insure that the
 stimulation that
 has been designed in the Methods \ref{stimulation-design}
 actually brings the neuron to the desired values
 of ($\mu_V, \sigma_V, \tau_V, \mu_G$), expected values
 are plain lines. Color code as in \textbf{A}. We performed the same
 numerical simulations than \textbf{A} for the subthreshold dynamics
 only (removing the threshold and reset mechanism) and we
 measure the four variables ($\mu_G$ is measured from the
 response to a short current step on top of the background activity,
 hence the noisy  behavior). 
 \textbf{(C)} Firing rate response of a single pyramidal neuron
in the ($\mu_V, \sigma_V$) plane. Data points and fit with
the template Equation \ref{eq-template} (linear \emph{phenomenological 
theshold} in inset). Errorbars represent standard deviation
 across two trials of different seed
 lasting 5 seconds each. \textbf{(D)} Measurements of the
 subthreshold variables after having clipped spikes, note that 
the deviations between desired and measured $\mu_V$, $\sigma_V$
 and $\tau_V$ are stronger for high firing level,
 i.e. potentially result from the bias introduced by 
the clipping procedure. \textbf{(E)} Firing rate response of three
 other neocortical cells in the ($\mu_V, \sigma_V$) plane.
}
\label{fig-muV-sV}
\end{figure*}
#+END_LATEX


 We first investigate here the firing rate response as a function of
the ($\mu_V, \sigma_V$) variables. The stimulation designed in the
Methods [[stimulation-design]] allows to vary ($\mu_V, \sigma_V$)
independently while keeping $\tau_V$ and $\mu_G$ constant, we check on
Figure [[fig-muV-sV][2]]B (in theoretical models by removing the spiking mechanism)
and Figure [[fig-muV-sV][2]]D (by clipping spikes in the intracellular recordings)
that the stimulation actually constrains $\mu_V$ and $\sigma_V$ . We
show on Figure [[fig-muV-sV][2]]A the response of the IaF model. As expected given the
strong non linearity of the threshold mechanism, the response is steep
as a function of the fluctuations size ($\sigma_V$) at depolarized
levels (high $\mu_V$) while the firing starts at higher $\sigma_V$ and
is much less steep for hyperpolarized levels (low $\mu_V$).

Introducing a linear function of $\mu_V$ and $\sigma_V$ for the
/phenomonological threshold/ (see inset of Figure [[fig-muV-sV][2]]A) was able to
accurately describe the firing rate response of the IaF model (thick
line in Figure [[fig-muV-sV][2]]A). The correction therefore reads:

\begin{equation}
  \label{eq-vthre-muV-sV}
  V_\mathrm{thre}^\mathrm{eff} = V_\mathrm{thre}^0 +
  P_{\mu_V} \, \frac{\mu_V - \mu_V^0}{\delta \mu_V^0} +
  P_{\sigma_V} \, 
  \frac{\sigma_V - \sigma_V^0}{\delta \sigma_V^0}
\end{equation}

 Here, to obtain comparable quantities, we have arbitrily normalized
 the dependency on $\mu_V$ and $\sigma_V$ around a mean configuration
 of the /fluctuation driven/ regime arbitrarily set to \(\mu_V^0=-55
 \mathrm{mV}\) and \(\sigma_V^0=4\mathrm{mV}\) and the extent of their
 domain \(\delta \mu_V^0=10 \mathrm{mV}\) and \(\delta
 \sigma_V^0=6\mathrm{mV}\). $V_\mathrm{thre}^0$, $P_{\mu_V}$ and
 $P_{\sigma_V}$ are the coefficients of the linear function.

 We next investigated the firing rate response of neocortical neurons
 as a function of the $\mu_V, \sigma_V$ variables (Figure [[fig-muV-sV][2]]C and
 Figure [[fig-muV-sV][2]]F). Again, an affine /phenomenological threshold/ (inset in
 Figure [[fig-muV-sV][2]]C) was found to be very accurate at capturing the observed
 firing rate response. The response of additional theoretical models
 and neocortical neurons in the ($\mu_V, \sigma_V$) plane is visible
 in Figure [[fig-3D][6]].

 An individual cellular behavior corresponds to a set of coefficients
 \(V_\mathrm{thre}^0\), $P_{\mu_V}$ and $P_{\sigma_V}$. We show on
 Figure [[fig-muV-sV][2]]F, the histogram of those coefficients across the recorded
 pyramidal cell population.

The first coefficient \(V_\mathrm{thre}^0\), account for a mean
threshold level, it represents the mean excitability level of the
neuron, we will see in the next section that it can depend on other
variables. The adaptative Exponential Integrate and Fire with Regular
Spiking features (AdExp-RS) shows a higher mean /phenomenological
threshold/ (see Figure [[fig-3D-space][6]]), indeed because of its finite sodium
activation curve and the firing adaptation phenomena, it is less
excitable that the IaF model. We see here that this mean excitability
level ($P_0$) shows a strong heterogeneity across the recorded
population, much stronger than what is predicted by only the finite
sampling of the irregular spiking process (see the Methods
[[finite-sampling]]).

The second coefficient \(P_{\mu_V}\) represents the deviation from the
behavior of Equation [[eq-template]] in the dependency to $\mu_V$. A
positive coefficient corresponds to an increasing /phenomenological
threshold/ with $\mu_V$ so to a reduction of the firing rate response
with respect to the template. All models show a positive coefficient
(see Figure [[fig-3D-space][6]]) so they all show an attenuated dependency with respect
to the template. This attenuation is stronger for the AdExp-RS model
due to 1) the adaptation mechanism (firing rate adaptation raises with
the firing rate which raises with $\mu_V$, so enhanced adaptation
decreases the dependency on $\mu_V$) and 2) the finite spike sharpness
that also lowers the excitability and therefore the sensitivity to
$\mu_V$ (see Figure [[fig-3D-space][6]]).

The third coefficient $P_{\sigma_V}$ represents the
deviation from the behavior of Equation \ref{eq-template} in the
dependency to $\sigma_V$. Again, a positive coefficient corresponds to
an increasing /phenomenological threshold/ with $\sigma_V$ so to a
reduction of the firing rate response with respect to the
template. Now the IaF model shows a negative coefficient (Figure [[fig-muV-sV][2]]F),
meaning the dependency on $\sigma_V$ is enhanced with respect to the
approximation. Here again (as for $\mu_V$), firing adaptation and
finite sharpness reduce firing rate raise with $\sigma_V$ as can be
seen for the AdExp-RS neuron (see Figure [[fig-3D-space][6]]).

\newpage


* Dependency on the speed of the subthreshold fluctuations 
:PROPERTIES:
:CUSTOM_ID: Tv
:END:

# --> FIGURE <-- #
#+BEGIN_LATEX
\begin{figure}[htb!]
\centering
\includegraphics[width=.4\linewidth]{fig4.pdf}
\caption{
\bfseries Sensitivity to the speed of the membrane potential fluctuations.
 \normalfont \textbf{(A)} Firing rate response as a function of
 slower fluctuations speed (increasing $\tau_V$) for 3 different models:
 the IaF model, the EIF model and the iLIF model. Their respective threshold
 have been changed to give them comparable excitabilities.
 \textbf{(B)} Mean, variance, 
 input conductance and \emph{global} autocorrelation time of
 the subthreshold  fluctuations in absence of a spike
 mechanism. A shift in the mean membrane potential
 has been made to bring the different neurons around
 the same firing rate level. \textbf{(C)} Firing rate
 dependency on the autocorrelation time for different
 pyramidal neurons and for different combinations
 of input. Experiments are indexed as a function
 of the mean output rate \textbf{(D)} Measured mean, variance
 and global autocorrelation time after clipping
 spikes for the experiments shown in
 \textbf{C} (same color code). Within one experiment, the
 couple ($\mu_V, \sigma_V$) should remain constant while
 $\tau_V$ should increase according to
 the dashed line. A strong shift is observed but the dependency of the deviations
 on the firing rate indicates that it is an effect of the clipping procedure
(see the color code, from red to 
 blue the firing rate raises as the deviations from the
 desired $\tau_V$).
 \textbf{(F)} Effective threshold
 for all data of \textbf{C} with the linear fit
 corresponding to Equation \ref{eq-Tv-shift}. \textbf{(F)} Histogram
 of the experimentally measured dependency
 of $V_\mathrm{thre}^\mathrm{eff}$ to $\tau_V$
 (i.e. $P_{\tau_V}$ coefficient) and comparison with
 the dependency of the models of \textbf{A}.}
\label{fig-Tv}
\end{figure}
#+END_LATEX

Because the firing rate is a temporal quantity, we expect a strong
dependency of the firing to the temporal dynamics properties of the
membrane potential fluctuations. It was shown in [[cite:Kuhn2004]] that
the firing rate can be greatly affected by the effective membrane time
constant $\tau_\mathrm{m}^{eff}$ for inputs leading to the same mean
$\mu_V$ and variance $\sigma_V$ for the subthreshold
fluctuations. Nevertheless, in this study, the temporal dynamics was
led by the membrane time constant and not by a mix of synaptic and
membrane time constants. Because synaptic time constant are not that
smaller from the effective membrane time constant (especially if we
consider the low-pass filtering exerted by dendritic trees), we choose
to relax this hypothesis and we investigate a domain of
autocorrelation where both the synaptic and the effective membrane
time constants would jointly contribute to the autocorrelation of the
membrane potential fluctuations. The definition of the /global
autocorrelation/ time considered in this study is presented in the
Methods [[autocorrel-def]].

 Here, the dynamic-clamp technique plays a crucial role, it allows to
 investigate values of the global autocorrelation that lie below the
 resting membrane time constant $\tau_\mathrm{m}^0$. Indeed, in the
 classical /current-clamp/ mode, when injecting stochastic processes
 (see [[cite:LaCamera2008]] for a review), the resting membrane time
 constant is a lower bound for the autocorrelation time. The injection
 of white noise will produce an Ornstein-Uhlenbeck noise of time
 constant $\tau_\mathrm{m}^0$ (under the single compartment
 approximation) and the injection of correlated noise will produce
 even higher autocorrelation values. Because /in vivo/, the temporal
 fluctuations are faster than the resting membrane time constant (see
 [[cite:Destexhe2003]] for a review) having an input that could
 reproduce this feature was crucial in our study.

 We used the expressions derived in the [[stimulation-design][Methods]] to design a
 stimulation keeping $\mu_V$, $\sigma_V$ and $\mu_G$ constant while
 increasing $\tau_V$. We tested this around a mean configuration of
 the /fluctuation-driven/ regime: \(\sigma_V=5\mathrm{mV}\), \(\mu_G=4
 g_\mathrm{L}\) and $\mu_V$ was set to obtain a mean firing rate
 between 1 and 15 Hz. The characteristics of the resulting
 subthreshold fluctuations can be seen for single compartment model on
 Figure [[fig-Tv][3]]B and for the data after clipping spikes in Figure [[fig:Tv][3]]D.

 We show on Figure [[fig:Tv][3]]A this relationship for three different models:
 the Iaf model, the EIF model with a sharpness of \(k_a=2\mathrm{mV}\)
 and the inactivating leaky Integrate and Fire model (iLIF,
 [[cite:Platkiewicz2011]]).

 As expected in a threshold crossing situation, faster fluctuations
 leads to higher firing rate than slow fluctuations, we thus observe a
 decreasing relationship between $\tau_V$ and the firing rate. This
 relation is however more or less pronounced as a function of the
 ability of the spiking mechanism to convert fast fluctuations into
 spikes. The spike sharpness creates this ability to track fast input,
 the reduced sharpness of the EIF model therefore result in a
 attenuated dependency to $\tau_V$ (see [[fig:Tv][Figure 3A]]).  A mechanism that
 penalizes the slow fluctuations also leads to an increased
 sensitivity to the speed of the fluctuations, the inactivation of
 sodium channels is such a mechanism. We show that adding an
 inactivation mechanism to the IaF model results in a stronger
 dependency to $\tau_V$ than the IaF model. This high impact of the
 inactivation mechanism appears because the fluctuations speed is very
 similar to the time constant of inactivation (\(\sim 5 \mathrm{ms}\))
 as would be expected /in vivo/.

 For the analytical description, we found that introducing a linear
 dependency on $\tau_V$ in the /phenomenological/ threshold was able
 to capture the observed behaviors. For convenience, the linear
 dependency is relative to the resting membrane time constant.

We introduce:
\begin{equation}
  \label{eq-Tv-shift}
  V_\mathrm{thre}^\mathrm{eff}=V_\mathrm{thre}^0 
  +P_{\tau_V^N} \, \frac{\tau_V^N-\tau_V^{N0}}{\delta \tau_V^{N0}}
\end{equation}

 where $P_{\tau}$ accounts for the threshold dependency induced by the
 behavior discussed above. The higher it is, the lower the $\tau_V$
 dependency (it smoothens the expected $\frac{1}{\tau_V}$
 dependency). Again, this dependency is normalized with respect to a
 mean configuration \(\tau_V^N = 0.5\) (i.e. \(\tau_V =
 \tau_\mathrm{m}^0 / 2 \) ) and the extent of the $\tau_V^N$
 variations: \(\delta \tau_V^N = 1\).

 This expression provides a quantitative way to evaluate the
 sensitivity to the speed of the fluctuations. Thus we investigated
 this sensitivity on several pyramidal neurons ([[fig-Tv][Figure 3C]]). It is
 striking to note that the mean behavior over the cells showed a
 remarkable sensitivity to the /global/ autocorrelation time, much
 stronger than the IaF model.

 As suggested by the theoretical models, this high sensitivity
 presumably results from the combination of 1) a high ability to track
 fast input, close to the IaF model [[cite:Naundorf2006]]
 [[cite:Ilin2013]] and 2) a mechanism that penalizes slow fluctuating
 input: the inactivation of sodium channels (again revealed by the use
 of the /dynamic-clamp/ technique that allows to produce fast membrane
 potential fluctuations, $\tau_V \sim 10 \mathrm{ms}$ where
 inactivation can have a critical role).


\newpage


* Dependency on the somatic input conductance
:PROPERTIES:
:CUSTOM_ID: muG
:END:

#+BEGIN_LATEX
\begin{figure}[htb!]
\centering
\includegraphics[width=.5\linewidth]{fig3.pdf}
\caption{\bfseries Increasing somatic input conductance shunts the
 sodium current and reduces spiking probability. 
\normalfont \textbf{(A)} Firing rate response as a
 response to the input varying only the total somatic
 conductance $\mu_G$ for the EIF model with three different
 spike shaprness \textbf{(B)} Insuring that the stimulation
 works. Mean, variance, input conductance and
 \textit{global autocorrelation time} of the subthreshold
 fluctuations in absence of a spike mechanism. A shift
 in the mean membrane potential has been made to bring
 the different neurons around the same firing rate
 level (they have quite different sensitivity
 levels). \textbf{(C)} Firing rate dependency on the total
 conductance for different pyramidal neurons and for different
 combinations of input. Experiments are indexed as a function
 of the mean output rate \textbf{(D)} Measured mean, variance
 and global autocorrelation time after clipping spikes
 for the experiments shown in \textbf{C} (same color code). Within
 one experiment, the set of  ($\mu_V, \sigma_V, \tau_V$) should
 remain constant.\textbf{(F)} Effective threshold for all data
 of \textbf{C} with the affine fit corresponding
 to Equation \ref{eq-muG-shift}. \textbf{(F)} Histogram of the
 experimentally measured dependency
 of $V_\mathrm{thre}^\mathrm{eff}$ to $\mu_G$ (i.e.
 $P_{\mu_G}$ coefficient) and comparison with the dependency
 of the three models of \textbf{A}.}
\label{fig-muG}
\end{figure}
#+END_LATEX

In neocortical neurons, the spike is produced by a sodium current
abruptly activated by membrane depolarisation. Under /in vivo/
conditions, the somatic input conductance is greatly increased as a
consequence of synaptic activity (see [[cite:Destexhe2003]] for a
review). Because the depolarization induced by the sodium current
depends on the input conductance, it is an important question to
evaluate how much the shunting of the sodium current reduces the
cellular excitability as a function of an increased input conductance
in the /fluctuation-driven/ regime.

The minimal model exhibiting this feature is the Exponential Integrate
and Fire (EIF) model. We can vary the sharpness of the spike
initiation current from an infinitly sharp current
($k_a=0\mathrm{mV}$, IaF model), to a rather smooth spiking current
($k_a=4\mathrm{mV}$), see Figure [[fig-muG][4]]A. We clearly see that the spike
initiation sharpness creates a decreasing dependency on the input
conductance for the firing rate level.

In [[cite:Platkiewicz2010]], in the context of their /threshold equation/,
the authors proposed a way to account for this decreased
excitability. We found that the mathematical expression that they
proposed:

\begin{equation}
  \label{eq-muG-shift}
  V_\mathrm{thre}^\mathrm{eff} = V_\mathrm{thre}^0 + P_{G} \cdot 
  \log \left( \frac{\mu_G}{g_\mathrm{L}} \right)
\end{equation}

was a good way to account for the dependency on the input conductance
in our /phenomenological threshold/ when introduced into the template
Equation [[eq-template]] (note that $P_{G}$ is
different from $k_a$ for the EIF model, because our /phenomenological
threshold/ does not correspond to the mathematically well-defined
threshold of [[cite:Platkiewicz2010]]). 

We now investigate this dependency in neocortical neurons by
artificial conductance increase using the /dynamic-clamp/ technique
(see the [[dynamic-clamp][Methods]]). In Figure [[fig-muG][4]]C, we tested the impact of an increased
input conductance at the soma on several pyramidal neurons. The
accuracy of the linear description for the /phenomenological
threshold/ as a function of $\log(\mu_G/g_\mathrm{L})$ is shown on
Figure [[fig-muG][4]]E for all recorded cells. The value of all the fitted
coefficients for $P_{\mu_G}$ can be seen in Figure [[fig-muG][4]]F.

It is striking to note that the mean behavior of neocortical cells can
should explained by a very smooth activation curve in a single
compartment model (thick black curve in Figure [[fig-muG][4]]F), similar to the
sodium activation curve obtained under voltage-clamp measurements
(*ref?*). In addition, unlike the depdency on $\tau_V$ (Figure [[fig-Tv][3]]F),
much of the observed variability can be explained by the finite
sampling of the irregular spiking process (Figure [[fig-muG][4]]F) suggesting that
this feature is rather homogeneously shared within the recorded
population.


\newpage


* Interplay of a conductance increase and faster fluctuations
:PROPERTIES:
:CUSTOM_ID: muG-Tv
:END:

# --> FIGURE <-- #
#+BEGIN_LATEX
\begin{figure}[htb!]
\centering
\includegraphics[width=.5\linewidth]{fig5.pdf}
\caption{
 \textbf{Firing rate as a response to an increasing input conductance
 and a decreasing membrane potential fluctuations speed.}
 \textbf{(A)} Firing rate response 
 for  the EIF model with three different
spike sharpness ($k_a=0\mathrm{mV}$ IaF, $k_a=1\mathrm{mV}$ 
and $k_a=2\mathrm{mV}$). The \emph{phenomenological threshold}
can be seen in the inset.
 \textbf{(B)} Mean, variance, 
input conductance and
\emph{global} autocorrelation time of the subthreshold
fluctuations in absence of a spike mechanism. A shift in the mean
membrane potential has been made to bring the different neurons
around the same firing rate level. In the inset, the autocorrelation 
function are visible as a function of the input conductance (color code)
\textbf{(C)} Response of different
neocortical neurons around $\sigma_V=5\mathrm{mV}$, 
$\tau_S / \tau_\mathrm{m}^0=15 \%$. The $\mu_V$ level was adjusted 
to bring the neuron in the 0-15 Hz domain.
 \textbf{(D)} Subthreshold variables when clipping
spikes, here $\tau_V/\tau_m^0 = \tau_S/\tau_m^0 + 1/\mu_G$ 
\textbf{(E)}
Corresponding effective thresholds. The dashed line corresponds to
the mean observed dependency.
\textbf{(F)} Histogram of the dependencis see in \textbf{E} with 
the dependencies of the theoretical models}
\label{fig-muG-Tv}
\end{figure}
#+END_LATEX

   In the two previous sections, we have investigated independently
   the dependency on the input conductance and the autocorrelation. A
   more physiological situation would correspond to a comodulation of
   $\mu_G$ and $\tau_V$. Indeed, when presynaptic activity raises for
   fixed synaptic time constants (we discard the potential effects on
   $\mu_V$ and $\sigma_V$), the somatic input conductance increases
   and the /global autocorrelation/ time decreases (Equation
   \ref{eq-Tv}, if \(\tau_S/\tau_m^0=\alpha=cst\) and $\mu_G$ varies,
   then $\tau_V/\tau_m^0 = \alpha + 1/\mu_G$). In the following for
   this comodulation, we will investigate this comodulation for
   \(\tau_S/\tau_m^0 \sim 0.1\).

   # The firing rate response of a neuron to a balanced input is of
   # significant importance because it is the quantity that is used in
   # the description of the activity of balanced network. In particular,
   # it was shown that the non-monotonic firing rate response (for
   # conductance-based synapses) leading to a self consistent
   # input-output value could predict the existence of a self sustained
   # activity point in the dynamics of sparse random networks
   # ([[cite:Kuhn2004]], [[cite:Kumar2008]]).

  The two previous sections predict opposite effects as a response to
  this type of comodulation. Increasing conductance reduces the firing
  rate for non infinitely sharp activation curves and faster temporal
  fluctuations increase the firing rate. It is therefore important to
  understand what is the final output of the combination of those two
  effects.

  For the IaF neuron, the effect is clear, the spiking mechanism does
  not create a dependency on $\mu_G$ then thre response to this
  comodulation result from the decrease of the /global
  autocorrelation/ an leads to an increase of the firing rate
  (reversing Figure [[fig-Tv][3]]A). On the other hand, for the EIF models, their
  dependency on $\tau_V$ is much weaker (EIF model on Figure [[fig-Tv][3]]A), so
  that the competition with the decreasing dependency on $\mu_G$ leads
  to the almost cancellation (EIF model $k_a=2\mathrm{mV}$) of this
  increase.

  We have run the same protocol on neocortical neurons, we found that
  the response to this comodulation is systematically increasing,
  still showing a high sensitivity to the speed of the fluctuations
  despite the potential dampening of the input conductance increase
  (see Figure [[fig-muG-Tv][5]]).

  # This information is redundant with the two previous protocols: the
  # dependency on $\mu_G$ is much weaker than the dependency to
  # $\tau_V$. When one computes $V_\mathrm{thre}^\mathrm{eff}$ with the
  # coefficients determined from the two previous sections and plug it
  # in Equation \ref{eq-template}, the firing rate response is led by
  # the dependency on $\tau_V$. But an experimental confirmation support
  # the self-consistency of our approach and the reliability of our
  # protocols.
  
  As this comodulation is likely to be the physiologically relevant
  case (though we could imagine situations where those values could
  vary independently, e.g. increase $\tau_V$ without $\mu_G$ by
  enhancing the proportion of low pass filtered distal input), we will
  use this to reduce the four-dimensional space to a three-dimensional
  space. Now variations of $\tau_V^N$ are set by varying the input
  conductance $\mu_G$ for a fixed \(\tau_S/\tau_\mathrm{m}^0\) i.e. we
  have: \( \tau_V/\tau_\mathrm{m}^0 = \tau_S/\tau_\mathrm{m}^0 +
  g_\mathrm{L}/\mu_G \). Again in the following, we will set
  \(\tau_S/\tau_\mathrm{m}^0 \sim 0.1\).


\newpage


* Full data for the 3 dimensional analysis

#+BEGIN_LATEX
\begin{figure*}[htb!]
\centering
\includegraphics[width=.07\linewidth]{../3d_scan/data/0.png}
\includegraphics[width=.07\linewidth]{../3d_scan/data/1.png}
\includegraphics[width=.07\linewidth]{../3d_scan/data/2.png}
\includegraphics[width=.07\linewidth]{../3d_scan/data/3.png}
\includegraphics[width=.07\linewidth]{../3d_scan/data/4.png}\\
\includegraphics[width=.07\linewidth]{../3d_scan/data/5.png}
\includegraphics[width=.07\linewidth]{../3d_scan/data/6.png}
\includegraphics[width=.07\linewidth]{../3d_scan/data/7.png}
\includegraphics[width=.07\linewidth]{../3d_scan/data/8.png}
\includegraphics[width=.07\linewidth]{../3d_scan/data/9.png}\\
\includegraphics[width=.07\linewidth]{../3d_scan/data/10.png}
\includegraphics[width=.07\linewidth]{../3d_scan/data/11.png}
\includegraphics[width=.07\linewidth]{../3d_scan/data/12.png}
\includegraphics[width=.07\linewidth]{../3d_scan/data/13.png}
\includegraphics[width=.07\linewidth]{../3d_scan/data/14.png}\\
\includegraphics[width=.07\linewidth]{../3d_scan/data/15.png}
\includegraphics[width=.07\linewidth]{../3d_scan/data/16.png}
\includegraphics[width=.07\linewidth]{../3d_scan/data/17.png}
\includegraphics[width=.07\linewidth]{../3d_scan/data/18.png}
\includegraphics[width=.07\linewidth]{../3d_scan/data/19.png}\\
\includegraphics[width=.07\linewidth]{../3d_scan/data/20.png}
\includegraphics[width=.07\linewidth]{../3d_scan/data/21.png}
\includegraphics[width=.07\linewidth]{../3d_scan/data/22.png}
\includegraphics[width=.07\linewidth]{../3d_scan/data/23.png}
\includegraphics[width=.07\linewidth]{../3d_scan/data/24.png}\\
\caption{\bfseries Full dataset (n=24) for the analysis in the 
 ($\mu_V, \sigma_V, \tau_V'$) space.
See Section \ref{3D-space-response} \normalfont .}
\label{fig-full-3D-data}
\end{figure*}
#+END_LATEX

\newpage


* Analysis of the fitted coefficients
:PROPERTIES:
:CUSTOM_ID: fitting-coefficients
:END:

# --> FIGURE <-- #
#+BEGIN_LATEX
\begin{figure}[htb!]
\centering
\includegraphics[width=\linewidth]{fig_3d_coeff.pdf}
\caption{\bfseries Fitted coefficients for theoretical 
models and individual cells (generating Figure \ref{fig-3D}).
\normalfont \textbf{(A)} PCA analysis of data. 
\textbf{(B)} Cross product of first component of the data
with those of theoretical models of varying parameters.}
\label{fig-heterogeneity}
\end{figure}
#+END_LATEX


\newpage


* Robustness of the firing rate characterization


The minimal number of points in the dataset of /long/ recordings
(presented in [[fig-full-3D-data]]) is n=40 points (meaning there has been
40 episodes of a given seed and a given ($\mu_V, \sigma_V, \tau_V$).

 Here, we investigate how reliable is the characterization over this
  limited number of points. We do this by taking 

the cell having the lowest number of points


 we split the dataset into two and we check whether the sensitivity is unchanged !!

# --> FIGURE <-- #
#+BEGIN_LATEX
\begin{figure}[htb!]
\centering
\includegraphics[width=.9\linewidth]{../figures/fitting_robustness.pdf}\\
\includegraphics[width=.9\linewidth]{../figures/fitting_robustness2.pdf}
\caption{\bfseries Fitting robustness. \normalfont
We split the dataset into two halves (for the cells having 
more than 75 points) and we compare the excitability and sensitivity
given by the two independent fits.}
\label{fig-heterogeneity}
\end{figure}
#+END_LATEX


\newpage

*** analysis :noexport:

#+begin_src python
N_MIN = 70 # 30 points minimum !!!

import sys
import matplotlib.pylab as plt
import numpy as np
from scipy.stats.stats import pearsonr

sys.path.append('../theoretical_tools/')
from encoding_power import get_mean_encoding_power
from template_and_fitting import erfc_func, fitting_Vthre_then_Fout,\
    final_threshold_func, print_reduce_parameters

sys.path.append('../3d_scan/')
from fit_Fout_response_of_data import load_full_data_set

sys.path.append('../experimental_data/')
from funcs_for_exp_analysis import load_reformated_data
## importing data
import dataset_structure as data
FourD_list = data.FourD_list
print FourD_list[-4:]
sys.path.append('/home/yann/work/python_library/')
from my_graph import set_plot

DATA2, INDEX2 = load_full_data_set(reformat=False)
DATA, INDEX = [], []
for data, index in zip(DATA2, INDEX2):
    if len(data[4])>N_MIN and index!='cell44' and index!='cell12':
        DATA.append(data)
        INDEX.append(index)
        print index, len(data[4])

E0_1, EmuV_1, EsV_1, ETv_1 = [], [], [], []
E0_2, EmuV_2, EsV_2, ETv_2 = [], [], [], []
i=0
for data in DATA:
    muV, sV, Tv_ratio, muGn, Fout, s_Fout, Vthre_eff, Gl, Cm, El,\
            muV_exp, sV_exp, Tv_exp, s_muV_exp, s_sV_exp, s_Tv_exp =\
            data

    i0 = int(len(data[0])/2.)

    ### first slice
    P1 = fitting_Vthre_then_Fout(Fout[:i0], 1e-3*muV[:i0],\
            1e-3*sV[:i0], Tv_ratio[:i0],\
            muGn[:i0], Gl, Cm, El, dep_muG=False, print_things=False)
    E01, EmuV1, EsV1, ETv1 = get_mean_encoding_power(P1, El, Gl, Cm)

    ### first slice
    P2 = fitting_Vthre_then_Fout(Fout[i0:], 1e-3*muV[i0:],\
            1e-3*sV[i0:], Tv_ratio[i0:],\
            muGn[i0:], Gl, Cm, El, dep_muG=False, print_things=False)
    E02, EmuV2, EsV2, ETv2 = get_mean_encoding_power(P2, El, Gl, Cm)
    
    if np.isfinite([E01, EmuV1, EsV1, ETv1, E02, EmuV2, EsV2, ETv2]).all():
        E0_1.append(E01);EmuV_1.append(EmuV1);EsV_1.append(EsV1);ETv_1.append(ETv1)
        E0_2.append(E02);EmuV_2.append(EmuV2);EsV_2.append(EsV2);ETv_2.append(ETv2)

    if EmuV1>1.6:
        print i, INDEX[i], len(data[4])
    i+=1

fig, ax = plt.subplots(1,4,figsize=(18,5))
plt.subplots_adjust(wspace=.4, bottom=.25, right=.99)
i=0
LABELS = [r"$\langle V_\mathrm{thre}^\mathrm{eff} \rangle_\mathcal{D}$",\
             r"$\langle d \nu / d \mu_V \rangle_\mathcal{D}$",\
                r"$\langle d \nu / d \sigma_V \rangle_\mathcal{D}$",\
                r"$\langle d \nu / d \tau_V^{N}' \rangle_\mathcal{D}$"]
'' 
for x, y, label in zip([E0_1, EmuV_1, EsV_1, ETv_1], [E0_2, EmuV_2, EsV_2, ETv_2], LABELS):
     x = np.array(x)
     y = np.array(y)
     cc, pp = pearsonr(x, y)
     ax[i].plot(x,y, 'kD')
     xth = np.linspace(x.min(), x.max())
     ax[i].plot(xth, np.polyval(np.polyfit(x, y, 1), xth), 'k--')
     ax[i].annotate('c='+str(np.round(cc,1))+', p='+'%.1e' % pp,\
                    (0.1,1.03), xycoords='axes fraction', fontsize=17)
     set_plot(ax[i], xlabel=label+'\n first slice', ylabel=label+'\n second slice')
     i+=1

fig.savefig('../figures/fitting_robustness.pdf')

#+end_src

#+RESULTS:
: None



*** analysis with points matching 				   :noexport:

#+begin_src python
N_MIN = 70 # 30 points minimum !!!

import sys
import matplotlib.pylab as plt
import numpy as np
from scipy.stats.stats import pearsonr

sys.path.append('../theoretical_tools/')
from encoding_power import get_mean_encoding_power
from template_and_fitting import erfc_func, fitting_Vthre_then_Fout,\
    final_threshold_func, print_reduce_parameters

sys.path.append('../3d_scan/')
from fit_Fout_response_of_data import load_full_data_set

sys.path.append('../experimental_data/')
from funcs_for_exp_analysis import load_reformated_data
## importing data
import dataset_structure as data
FourD_list = data.FourD_list
print FourD_list[-4:]
sys.path.append('/home/yann/work/python_library/')
from my_graph import set_plot

DATA2, INDEX2 = load_full_data_set(reformat=False)
DATA, INDEX = [], []
for data, index in zip(DATA2, INDEX2):
    if len(data[4])>N_MIN and index!='cell44' and index!='cell12':
        DATA.append(data)
        INDEX.append(index)
        print index, len(data[4])

E0_1, EmuV_1, EsV_1, ETv_1 = [], [], [], []
E0_2, EmuV_2, EsV_2, ETv_2 = [], [], [], []
i=0
for data in DATA:
    muV, sV, Tv_ratio, muGn, Fout, s_Fout, Vthre_eff, Gl, Cm, El,\
            muV_exp, sV_exp, Tv_exp, s_muV_exp, s_sV_exp, s_Tv_exp =\
            data

    muV_unique, sV_unique, Tv_unique = np.unique(muV), np.unique(sV),\
                                       np.unique(Tv_ratio)
    muV1, sV1, Tv1, Fout1 = [], [], [], []
    muV2, sV2, Tv2, Fout2 = [], [], [], []
    for m,s,t in zip(muV_unique, sV_unique, Tv_unique):
        ii = np.where((muV==m) & (sV==s) & (t==Tv_ratio))[0]
        if len(ii)>1:
            muV1.append(muV[ii[0]]);sV1.append(muV[ii[0]])
            Tv1.append(Tv_ratio[ii[0]]);Fout1.append(Fout[ii[0]])
            muV2.append(muV[ii[1]]);sV2.append(muV[ii[1]])
            Tv2.append(Tv_ratio[ii[1]]);Fout2.append(Fout[ii[1]])

    muV1, sV1, Tv1, Fout1 = np.array(muV1), np.array(sV1), np.array(Tv1), np.array(Fout1)
    muV2, sV2, Tv2, Fout2 = np.array(muV2), np.array(sV2), np.array(Tv2), np.array(Fout2)
    ### first slice
    P1 = fitting_Vthre_then_Fout(Fout1, 1e-3*muV1,\
            1e-3*sV1, Tv1,\
            0*Tv1, Gl, Cm, El, dep_muG=False, print_things=False)
    E01, EmuV1, EsV1, ETv1 = get_mean_encoding_power(P1, El, Gl, Cm)

    ### first slice
    P2 = fitting_Vthre_then_Fout(Fout2, 1e-3*muV2,\
            1e-3*sV2, Tv2,\
            0*Tv2, Gl, Cm, El, dep_muG=False, print_things=False)
    E02, EmuV2, EsV2, ETv2 = get_mean_encoding_power(P2, El, Gl, Cm)
    
    if np.isfinite([E01, EmuV1, EsV1, ETv1, E02, EmuV2, EsV2, ETv2]).all():
        E0_1.append(E01);EmuV_1.append(EmuV1);EsV_1.append(EsV1);ETv_1.append(ETv1)
        E0_2.append(E02);EmuV_2.append(EmuV2);EsV_2.append(EsV2);ETv_2.append(ETv2)

    if EmuV1>1.6:
        print i, INDEX[i], len(data[4])
    i+=1

fig, ax = plt.subplots(1,4,figsize=(18,5))
plt.subplots_adjust(wspace=.4, bottom=.25, right=.99)
i=0
LABELS = [r"$\langle V_\mathrm{thre}^\mathrm{eff} \rangle_\mathcal{D}$",\
             r"$\langle d \nu / d \mu_V \rangle_\mathcal{D}$",\
                r"$\langle d \nu / d \sigma_V \rangle_\mathcal{D}$",\
                r"$\langle d \nu / d \tau_V^{N}' \rangle_\mathcal{D}$"]
'' 
for x, y, label in zip([E0_1, EmuV_1, EsV_1, ETv_1], [E0_2, EmuV_2, EsV_2, ETv_2], LABELS):
     x = np.array(x)
     y = np.array(y)
     cc, pp = pearsonr(x, y)
     ax[i].plot(x,y, 'rD')
     xth = np.linspace(x.min(), x.max())
     ax[i].plot(xth, np.polyval(np.polyfit(x, y, 1), xth), 'r--')
     ax[i].annotate('c='+str(np.round(cc,1))+', p='+'%.1e' % pp,\
                    (0.1,1.03), xycoords='axes fraction', fontsize=17)
     set_plot(ax[i], xlabel=label+'\n first slice', ylabel=label+'\n second slice')
     i+=1

fig.savefig('../figures/fitting_robustness2.pdf')

#+end_src

#+RESULTS:
: None



* Nature of the observed heterogeneity, underlying structure ?
:PROPERTIES:
:CUSTOM_ID: heterogeneity
:END:

[...] to be written

# --> FIGURE <-- #
#+BEGIN_LATEX
\begin{figure}[htb!]
\centering
\includegraphics[width=.6\linewidth]{fig8.pdf}
\caption{\bfseries Characteristics of the measured heterogenity.
\normalfont \textbf{(A)} PCA analysis of data. 
\textbf{(B)} Cross product of first component of the data
with those of theoretical models of varying parameters.}
\label{fig-heterogeneity}
\end{figure}
#+END_LATEX


\newpage



* Correlation between experimental conditions and functional properties

Cross correlation between the quantities calculated and presented in
[[exp-details]], with the functional quantities measured...


* Theoretical models matching the experimental response 

in the 5d params scan of the prameters, what is the model that
minimize the difference with the data, what is the change of
parameters that


* Comparison of different strategy to capture the firing rate response :noexport:
:PROPERTIES:
:CUSTOM_ID: other-strategies
:END:

# \begin{enumerate}
# \item The constant driving force approximation.\\
#   Considering $E_{\{e,i\}} - V(t) \simeq E_{\{e,i\}} - \mu_V$ as a
#   constant allows to reduce the conductance-based problem to a
#   current-based problem. The two excitatory and inhibitory currents
#   sum linearly and can then be merged into a total input current
#   $I(t)$ and will be able to apply the formulas based on the first
#   passage time (see \ref{sec:brunel-comp}). Then comes the
#   question of the temporal correlation of the total current $I(t)$,
#   both $g_E(t)$ and $g_I(t)$ are temporally correlated (they are
#   O.U. processes), what is the autocorrelation time of a linear sum of
#   both ?. As the processes are uncorrelated, we hypothesize that the
#   resulting current will show no temporal correlation (i.e. is a white
#   noise), so that we can directly apply the formula for the first
#   passage time of the Langevin process.
# \item The \textit{crude approximation} of Amit \& Brunel without correction.
# \end{enumerate}

# \begin{figure}[h!]
#   \centering
#   \caption{We compare the firing rate approximations to the numerical
#   realisation. See details in \ref{sec:brunel-comp} }
# \label{fig:brunel-comp}
# \end{figure}


* Higher order terms in the stimulation :noexport:
:PROPERTIES:
:CUSTOM_ID: higher-order-in stim
:END:

Figure with PSP event shapes where we illustrate that for the IaF
neuron, you slightly increase the firing probability for the high
conductance stimuli. Probably the amplitude of the variations around
the constant level is a way to assess the impact of the varying higher
order terms resulting from the stimulation choice. For example, here
we fix ($\mu_V, \sigma_V, \tau_V$) and we vary $\tau_V$ thanks to
[[eq-final-input]] and [[eq-conversion-rule]]. But by making this, we also
vary higher order terms in an unknown fashion. We can see that the
variations of those terms create a slight increase of the firing
rate. Nevertheless the variations of those higher order terms have
only an impact of $\sim$ 0.2Hz around 7Hz).



* References

#+BEGIN_LATEX
\begin{filecontents}{biblio.bib}

@article{Chance2002,
abstract = {Gain modulation is a prominent feature of neuronal activity recorded in behaving animals, but the mechanism by which it occurs is unknown. By introducing a barrage of excitatory and inhibitory synaptic conductances that mimics conditions encountered in vivo into pyramidal neurons in slices of rat somatosensory cortex, we show that the gain of a neuronal response to excitatory drive can be modulated by varying the level of "background" synaptic input. Simultaneously increasing both excitatory and inhibitory background firing rates in a balanced manner results in a divisive gain modulation of the neuronal response without appreciable signal-independent increases in firing rate or spike-train variability. These results suggest that, within active cortical circuits, the overall level of synaptic input to a neuron acts as a gain control signal that modulates responsiveness to excitatory drive.},
author = {Chance, Frances S. and Abbott, L. F. and Reyes, Alex D.},
doi = {10.1016/S0896-6273(02)00820-6},
file = {:home/yann/Documents/Mendeley/Chance, Abbott, Reyes - 2002 - Gain modulation from background synaptic input.pdf:pdf},
isbn = {0896-6273 (Print)$\backslash$n0896-6273 (Linking)},
issn = {08966273},
journal = {Neuron},
mendeley-groups = {Neuroscience},
pages = {773--782},
pmid = {12194875},
title = {{Gain modulation from background synaptic input}},
volume = {35},
year = {2002}
}

@article{Tuckwell2002,
author = {Tuckwell, Henry C and Wan, Frederic Y M and Rospars, Jean-Pierre},
file = {:C$\backslash$:/Users/yann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tuckwell, Wan, Rospars - 2002 - A spatial stochastic neuronal model with Ornstein--Uhlenbeck input current.pdf:pdf},
journal = {Biological cybernetics},
mendeley-groups = {Neuroscience},
number = {2},
pages = {137--145},
publisher = {Springer},
title = {{A spatial stochastic neuronal model with Ornstein--Uhlenbeck input current}},
volume = {86},
year = {2002}
}

@article{Debanne2011,
abstract = {Axons are generally considered as reliable transmission cables in which stable propagation occurs once an action potential is generated. Axon dysfunction occupies a central position in many inherited and acquired neurological disorders that affect both peripheral and central neurons. Recent findings suggest that the functional and computational repertoire of the axon is much richer than traditionally thought. Beyond classical axonal propagation, intrinsic voltage-gated ionic currents together with the geometrical properties of the axon determine several complex operations that not only control signal processing in brain circuits but also neuronal timing and synaptic efficacy. Recent evidence for the implication of these forms of axonal computation in the short-term dynamics of neuronal communication is discussed. Finally, we review how neuronal activity regulates both axon morphology and axonal function on a long-term time scale during development and adulthood.},
author = {Debanne, Dominique and Campanac, Emilie and Bialowas, Andrzej and Carlier, Edmond and Alcaraz, Gis\`{e}le},
doi = {10.1152/physrev.00048.2009},
file = {:C$\backslash$:/Users/yann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Debanne et al. - 2011 - Axon physiology.pdf:pdf},
issn = {1522-1210},
journal = {Physiological reviews},
keywords = {Action Potentials,Action Potentials: physiology,Animals,Axons,Axons: pathology,Axons: physiology,Cell Proliferation,Channelopathies,Channelopathies: pathology,Electrophysiological Phenomena,Humans,Ion Channels,Ion Channels: physiology,Neuronal Plasticity,Neuronal Plasticity: physiology,Signal Transduction,Signal Transduction: physiology,Synaptic Transmission,Synaptic Transmission: physiology},
mendeley-groups = {Neuroscience},
month = apr,
number = {2},
pages = {555--602},
pmid = {21527732},
title = {{Axon physiology.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21527732},
volume = {91},
year = {2011}
}

@incollection{Lippiat2008,
year={2009},
isbn={978-1-934115-65-7},
booktitle={Potassium Channels},
volume={491},
series={Methods in Molecular Biology},
editor={Lippiat, JonathanD.},
doi={10.1007/978-1-59745-526-8_11},
title={Whole-Cell Recording Using the Perforated Patch Clamp Technique},
url={http://dx.doi.org/10.1007/978-1-59745-526-8_11},
publisher={Humana Press},
keywords={Amphotericin B; Nystatin; Patch clamp; Perforated patch; Potassium channels},
author={Lippiat, JonathanD.},
pages={141-149},
language={English}
}

@article{Rae1991,
author = {Rae, James and Cooper, Kim and Gates, Peter and Watsky, Mitchell},
doi = {10.1016/0165-0270(91)90017-T},
file = {:home/yann/Documents/Mendeley/Rae et al. - 1991 - Low access resistance perforated patch recordings using amphotericin B.pdf:pdf},
issn = {01650270},
journal = {Journal of Neuroscience Methods},
keywords = {amphotericin,nystatin,patch clamp,perforated patch,single channels,whole cell currents},
mendeley-groups = {Neuroscience},
month = mar,
number = {1},
pages = {15--26},
title = {{Low access resistance perforated patch recordings using amphotericin B}},
url = {http://linkinghub.elsevier.com/retrieve/pii/016502709190017T},
volume = {37},
year = {1991}
}

@article{Wendt1992,
abstract = {The results of studies on modulation of Na channel function are often difficult to interpret due to time-dependent changes in channel kinetics. Although the "tight-seal" whole cell voltage-clamp technique has proved very useful in studying the properties of the cardiac Na current, the spontaneous shift of parameters of inactivation and activation gating to more negative potential is a serious limitation to the use of the technique. The shifts are believed to result from changes in the intracellular milieu effected by dialysis; moreover, use of a variety of different anions and cations in the internal micropipette solution has not obviated the problem. The perforated-patch technique permits low-resistance intracellular access without free dialysis between the intracellular solution and the recording micropipette. We have compared steady-state inactivation and peak current-voltage relationship of whole cell Na currents measured with the conventional whole cell and perforated-patch techniques in rabbit atrial myocytes at 17 degrees C. Although gating parameters shifted to more negative potentials when recorded with the conventional technique, stable kinetics could be observed for up to 150 min with the perforated-patch technique. The potential for one-half Na channel inactivation was -73 +/- 5.1 mV and is consistent with measurements made using indirect techniques such as upstroke velocity measurements. The fact that the intracellular milieu is left relatively intact makes the approach attractive for studying modulation of the Na current by neurotransmitters and hormones.},
author = {Wendt, D J and Starmer, C F and Grant, A O},
isbn = {0002-9513},
issn = {0002-9513},
journal = {The American journal of physiology},
pages = {C1234--C1240},
pmid = {1335689},
title = {{Na channel kinetics remain stable during perforated-patch recordings.}},
volume = {263},
year = {1992}
}

@article{Kyrozis1995,
author = {Kyrozis, Andreas and Reichling, David B.},
doi = {10.1016/0165-0270(94)00116-X},
file = {:home/yann/Documents/Mendeley/Kyrozis, Reichling - 1995 - Perforated-patch recording with gramicidin avoids artifactual changes in intracellular chloride concentratio.pdf:pdf},
issn = {01650270},
journal = {Journal of Neuroscience Methods},
keywords = {amphotericin,chloride channel,gaela,glycine,gramicidin,perforated patch,spinal cord},
mendeley-groups = {Neuroscience},
month = mar,
number = {1},
pages = {27--35},
title = {{Perforated-patch recording with gramicidin avoids artifactual changes in intracellular chloride concentration}},
url = {http://linkinghub.elsevier.com/retrieve/pii/016502709400116X},
volume = {57},
year = {1995}
}

@article{Richardson2007,
author = {Richardson, Magnus J E},
file = {:home/yann/Documents/Mendeley/Richardson\_2007\_Firing-rate response of linear and nonlinear integrate-and-fire neurons to modulated current-based and conductance-based.pdf:pdf},
journal = {Physical Review E},
mendeley-groups = {Neuroscience},
number = {2},
pages = {21919},
publisher = {APS},
title = {{Firing-rate response of linear and nonlinear integrate-and-fire neurons to modulated current-based and conductance-based synaptic drive}},
volume = {76},
year = {2007}
}

@article{Brette2015,
abstract = {A large variety of neuron models are used in theoretical and computational neuroscience, and among these, single-compartment models are a popular kind. These models do not explicitly include the dendrites or the axon, and range from the Hodgkin-Huxley (HH) model to various flavors of integrate-and-fire (IF) models. The main classes of models differ in the way spikes are initiated. Which one is the most realistic? Starting with some general epistemological considerations, I show that the notion of realism comes in two dimensions: empirical content (the sort of predictions that a model can produce) and empirical accuracy (whether these predictions are correct). I then examine the realism of the main classes of single-compartment models along these two dimensions, in light of recent experimental evidence.},
author = {Brette, Romain},
doi = {10.1371/journal.pcbi.1004114},
file = {:home/yann/Documents/Mendeley/Brette\_2015\_What Is the Most Realistic Single-Compartment Model of Spike Initiation.pdf:pdf},
issn = {1553-7358},
journal = {PLoS computational biology},
mendeley-groups = {Neuroscience},
month = apr,
number = {4},
pages = {e1004114},
pmid = {25856629},
title = {{What Is the Most Realistic Single-Compartment Model of Spike Initiation?}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/25856629},
volume = {11},
year = {2015}
}

@article{Giugliano2008,
author = {Giugliano, Michele and {La Camera}, Giancarlo and Fusi, Stefano and Senn, Walter},
doi = {10.1007/s00422-008-0270-9},
file = {:C$\backslash$:/Users/yann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Giugliano et al. - 2008 - The response of cortical neurons to in vivo-like input current theory and experiment II. Time-varying and spat.pdf:pdf},
issn = {1432-0770},
journal = {Biological cybernetics},
keywords = {Animals,Cerebral Cortex,Cerebral Cortex: physiology,Humans,Models, Neurological,Neural Networks (Computer),Neurons,Neurons: physiology},
mendeley-groups = {Neuroscience},
month = nov,
number = {4-5},
pages = {303--18},
pmid = {19011920},
title = {{The response of cortical neurons to in vivo-like input current: theory and experiment: II. Time-varying and spatially distributed inputs.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/19011920},
volume = {99},
year = {2008}
}

@article{Destexhe2006,
author = {Destexhe, Alain and Contreras, Diego},
journal = {Science},
mendeley-groups = {Neuroscience},
number = {October},
pages = {85--90},
title = {{Neuronal computations with stochastic network states}},
url = {http://www.sciencemag.org/content/314/5796/85.short},
volume = {989},
year = {2006}
}

@article{Platkiewicz2011,
author = {Platkiewicz, Jonathan and Brette, Romain},
doi = {10.1371/journal.pcbi.1001129},
file = {:home/yann/Documents/Mendeley/Platkiewicz, Brette - 2011 - Impact of fast sodium channel inactivation on spike threshold dynamics and synaptic integration.pdf:pdf},
issn = {1553-7358},
journal = {PLoS computational biology},
keywords = {Action Potentials,Action Potentials: physiology,Animals,Cats,Cerebral Cortex,Cerebral Cortex: cytology,Databases, Factual,Models, Neurological,Neurons,Neurons: physiology,Patch-Clamp Techniques,Sodium Channels,Sodium Channels: physiology,Synapses,Synapses: physiology},
mendeley-groups = {Neuroscience,Neuroscience/Cellular Biophysics},
month = may,
number = {5},
pages = {e1001129},
pmid = {21573200},
title = {{Impact of fast sodium channel inactivation on spike threshold dynamics and synaptic integration.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3088652\&tool=pmcentrez\&rendertype=abstract},
volume = {7},
year = {2011}
}

@article{Tchumatchenko2011,
author = {Tchumatchenko, Tatjana and Malyshev, Aleksey and Wolf, Fred and Volgushev, Maxim},
doi = {10.1523/JNEUROSCI.2182-11.2011},
file = {:C$\backslash$:/Users/yann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tchumatchenko et al. - 2011 - Ultrafast population encoding by cortical neurons.pdf:pdf},
issn = {1529-2401},
journal = {The Journal of neuroscience : the official journal of the Society for Neuroscience},
keywords = {Action Potentials,Action Potentials: physiology,Animals,Cats,Excitatory Postsynaptic Potentials,Excitatory Postsynaptic Potentials: physiology,Female,Male,Mental Processes,Mental Processes: physiology,Models, Neurological,Neocortex,Neocortex: cytology,Neocortex: physiology,Organ Culture Techniques,Patch-Clamp Techniques,Patch-Clamp Techniques: methods,Pyramidal Cells,Pyramidal Cells: physiology,Rats,Rats, Wistar,Reaction Time,Reaction Time: physiology,Signal Processing, Computer-Assisted,Species Specificity,Time Factors,Visual Cortex,Visual Cortex: cytology,Visual Cortex: physiology,Visual Perception,Visual Perception: physiology},
mendeley-groups = {Neuroscience},
month = aug,
number = {34},
pages = {12171--9},
pmid = {21865460},
title = {{Ultrafast population encoding by cortical neurons.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=4225046\&tool=pmcentrez\&rendertype=abstract},
volume = {31},
year = {2011}
}

@article{Ilin2013,
author = {Ilin, Vladimir and Malyshev, Aleksey and Wolf, Fred and Volgushev, Maxim},
doi = {10.1523/JNEUROSCI.0771-12.2013},
file = {:C$\backslash$:/Users/yann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ilin et al. - 2013 - Fast computations in cortical ensembles require rapid initiation of action potentials.pdf:pdf},
issn = {1529-2401},
journal = {The Journal of neuroscience : the official journal of the Society for Neuroscience},
keywords = {Action Potentials,Action Potentials: physiology,Animals,Computer Simulation,Female,Male,Models, Neurological,Organ Culture Techniques,Rats,Rats, Wistar,Time Factors,Visual Cortex,Visual Cortex: physiology},
mendeley-groups = {Neuroscience},
month = feb,
number = {6},
pages = {2281--92},
pmid = {23392659},
title = {{Fast computations in cortical ensembles require rapid initiation of action potentials.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3964617\&tool=pmcentrez\&rendertype=abstract},
volume = {33},
year = {2013}
}

@article{Kuhn2004,
author = {Kuhn, Alexandre and Aertsen, Ad and Rotter, Stefan},
doi = {10.1523/JNEUROSCI.3349-03.2004},
issn = {1529-2401},
journal = {The Journal of neuroscience : the official journal of the Society for Neuroscience},
keywords = {Action Potentials,Action Potentials: physiology,Animals,Computer Simulation,Excitatory Postsynaptic Potentials,Excitatory Postsynaptic Potentials: physiology,Humans,Membrane Potentials,Membrane Potentials: physiology,Models,Neural Inhibition,Neural Inhibition: physiology,Neurological,Neurons,Neurons: physiology,Synapses,Synapses: physiology,Synaptic Transmission,Synaptic Transmission: physiology,Visual Cortex,Visual Cortex: physiology},
month = mar,
number = {10},
pages = {2345--56},
pmid = {15014109},
publisher = {Soc Neuroscience},
title = {{Neuronal integration of synaptic input in the fluctuation-driven regime.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15014109},
volume = {24},
year = {2004}
}

@article{Brunel2001a,
abstract = {Noise can have a significant impact on the response dynamics of a nonlinear system. For neurons, the primary source of noise comes from background synaptic input activity. If this is approximated as white noise, the amplitude of the modulation of the firing rate in response to an input current oscillating at frequency omega decreases as 1/square root[omega] and lags the input by 45 degrees in phase. However, if filtering due to realistic synaptic dynamics is included, the firing rate is modulated by a finite amount even in the limit omega-->infinity and the phase lag is eliminated. Thus, through its effect on noise inputs, realistic synaptic dynamics can ensure unlagged neuronal responses to high-frequency inputs.},
author = {Brunel, Nicolas and Chance, F S and Fourcaud, N and Abbott, L F},
issn = {0031-9007},
journal = {Physical review letters},
keywords = {Action Potentials,Action Potentials: physiology,Mathematical Computing,Models, Neurological,Neurons,Neurons: physiology,Synapses,Synapses: physiology,Synaptic Transmission,Synaptic Transmission: physiology},
mendeley-groups = {Neuroscience/Neuronal models},
month = mar,
number = {10},
pages = {2186--9},
pmid = {11289886},
title = {{Effects of synaptic noise and filtering on the frequency response of spiking neurons.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/11289886},
volume = {86},
year = {2001}
}

@article{Brunel1998a,
abstract = {We consider a model of an integrate-and-fire neuron with synaptic current dynamics, in which the synaptic time constant tau' is much smaller than the membrane time constant tau. We calculate analytically the firing frequency of such a neuron for inputs described by a random Gaussian process. We find that the first order correction to the frequency due to tau' is proportional to the square root of the ratio between these time constants radicaltau'/tau. This implies that the correction is important even when the synaptic time constant is small compared with that of the potential. The frequency of a neuron with tau'>0 can be reduced to that of the basic IF neuron (corresponding to tau'=1) using an "effective" threshold which has a linear dependence on radical tau'/tau. Numerical simulations show a very good agreement with the analytical result, and permit an extrapolation of the "effective" threshold to higher orders in radical tau'/tau. The obtained frequency agrees with simulation data for a wide range of parameters.},
author = {Brunel, Nicolas and Sergi, S},
doi = {10.1006/jtbi.1998.0782},
file = {:C$\backslash$:/Users/yann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Brunel, Sergi - 1998 - Firing frequency of leaky intergrate-and-fire neurons with synaptic current dynamics.pdf:pdf},
issn = {0022-5193},
journal = {Journal of theoretical biology},
keywords = {Animals,Computer Simulation,Membrane Potentials,Membrane Potentials: physiology,Models,Neurological,Neurons,Neurons: physiology,Synaptic Transmission},
mendeley-groups = {Neuroscience},
month = nov,
number = {1},
pages = {87--95},
pmid = {9802952},
title = {{Firing frequency of leaky intergrate-and-fire neurons with synaptic current dynamics.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/9802952},
volume = {195},
year = {1998}

}
@article{Brunel2014,
abstract = {At the single neuron level, information processing involves the transformation of input spike trains into an appropriate output spike train. Building upon the classical view of a neuron as a threshold device, models have been developed in recent years that take into account the diverse electrophysiological make-up of neurons and accurately describe their input-output relations. Here, we review these recent advances and survey the computational roles that they have uncovered for various electrophysiological properties, for dendritic arbor anatomy as well as for short-term synaptic plasticity.},
author = {Brunel, Nicolas and Hakim, Vincent and Richardson, Magnus J E},
doi = {10.1016/j.conb.2014.01.005},
file = {:C$\backslash$:/Users/yann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Brunel, Hakim, Richardson - 2014 - Single neuron dynamics and computation.pdf:pdf},
issn = {1873-6882},
journal = {Current opinion in neurobiology},
mendeley-groups = {Neuroscience},
month = jan,
pages = {149--155},
pmid = {24492069},
publisher = {Elsevier Ltd},
title = {{Single neuron dynamics and computation.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24492069},
volume = {25C},
year = {2014}
}

@article{Fourcaud-Trocme2003,
abstract = {This study examines the ability of neurons to track temporally varying inputs, namely by investigating how the instantaneous firing rate of a neuron is modulated by a noisy input with a small sinusoidal component with frequency (f). Using numerical simulations of conductance-based neurons and analytical calculations of one-variable nonlinear integrate-and-fire neurons, we characterized the dependence of this modulation on f. For sufficiently high noise, the neuron acts as a low-pass filter. The modulation amplitude is approximately constant for frequencies up to a cutoff frequency, fc, after which it decays. The cutoff frequency increases almost linearly with the firing rate. For higher frequencies, the modulation amplitude decays as C/falpha, where the power alpha depends on the spike initiation mechanism. For conductance-based models, alpha = 1, and the prefactor C depends solely on the average firing rate and a spike "slope factor," which determines the sharpness of the spike initiation. These results are attributable to the fact that near threshold, the sodium activation variable can be approximated by an exponential function. Using this feature, we propose a simplified one-variable model, the "exponential integrate-and-fire neuron," as an approximation of a conductance-based model. We show that this model reproduces the dynamics of a simple conductance-based model extremely well. Our study shows how an intrinsic neuronal property (the characteristics of fast sodium channels) determines the speed with which neurons can track changes in input.},
author = {Fourcaud, Nicolas and Hansel, David and van Vreeswijk, Carl and Brunel, Nicolas},
file = {:C$\backslash$:/Users/yann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fourcaud-Trocm\'{e} et al. - 2003 - How spike generation mechanisms determine the neuronal response to fluctuating inputs.pdf:pdf},
issn = {1529-2401},
journal = {The Journal of neuroscience},
keywords = {Action Potentials,Electric Conductivity,Kinetics,Models,Neurological,Neurons,Neurons: physiology,Sodium Channels,Sodium Channels: metabolism},
mendeley-groups = {Neuroscience},
month = dec,
number = {37},
pages = {11628--11640},
pmid = {14684865},
publisher = {Soc Neuroscience},
title = {{How spike generation mechanisms determine the neuronal response to fluctuating inputs}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/14684865},
volume = {23},
year = {2003}
}

@article{Brunel2000a,
abstract = {Recent advances in the understanding of the dynamics of populations of spiking neurones are reviewed. These studies shed light on how a population of neurones can follow arbitrary variations in input stimuli, how the dynamics of the population depends on the type of noise, and how recurrent connections influence the dynamics. The importance of inhibitory feedback for the generation of irregularity in single cell behaviour is emphasized. Examples of computation that recurrent networks with excitatory and inhibitory cells can perform are then discussed. Maintenance of a network state as an attractor of the system is discussed as a model for working memory function, in both object and spatial modalities. These models can be used to interpret and make predictions about electrophysiological data in the awake monkey.},
author = {Brunel, Nicolas},
file = {:C$\backslash$:/Users/yann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Brunel - 2000 - Dynamics of networks of randomly connected excitatory and inhibitory spiking neurons.pdf:pdf},
issn = {0928-4257},
journal = {Journal of Physiology-Paris},
keywords = {Acoustic Stimulation,Animals,Cerebral Cortex,Cerebral Cortex: physiology,Evoked Potentials,Excitatory Postsynaptic Potentials,Feedback,Humans,Interneurons,Interneurons: physiology,Membrane Potentials,Memory,Memory: physiology,Models,Nerve Net,Nerve Net: physiology,Neurological,Neurons,Neurons: physiology,Pyramidal Cells,Pyramidal Cells: physiology,Space Perception,Synapses,Synapses: physiology},
mendeley-groups = {Neuroscience},
number = {5},
pages = {445--463},
pmid = {11165912},
publisher = {Elsevier},
title = {{Dynamics of networks of randomly connected excitatory and inhibitory spiking neurons}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/11165912},
volume = {94},
year = {2000}
}

@article{Brunel1999,
abstract = {We study analytically the dynamics of a network of sparsely connected inhibitory integrate-and-fire neurons in a regime where individual neurons emit spikes irregularly and at a low rate. In the limit when the number of neurons --> infinity, the network exhibits a sharp transition between a stationary and an oscillatory global activity regime where neurons are weakly synchronized. The activity becomes oscillatory when the inhibitory feedback is strong enough. The period of the global oscillation is found to be mainly controlled by synaptic times but depends also on the characteristics of the external input. In large but finite networks, the analysis shows that global oscillations of finite coherence time generically exist both above and below the critical inhibition threshold. Their characteristics are determined as functions of systems parameters in these two different regions. The results are found to be in good agreement with numerical simulations.},
author = {Brunel, Nicolas and Hakim, Vincent},
file = {:C$\backslash$:/Users/yann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Brunel, Hakim - 1999 - Fast global oscillations in networks of integrate-and-fire neurons with low firing rates.pdf:pdf},
issn = {0899-7667},
journal = {Neural computation},
keywords = {Algorithms,Computer Simulation,Electrophysiology,Linear Models,Models,Neural Networks (Computer),Neurological,Neurons,Neurons: physiology,Nonlinear Dynamics,Synapses,Synapses: physiology},
mendeley-groups = {Neuroscience},
month = oct,
number = {7},
pages = {1621--1671},
pmid = {10490941},
publisher = {MIT Press},
title = {{Fast global oscillations in networks of integrate-and-fire neurons with low firing rates}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/10490941},
volume = {11},
year = {1999}
}

@article{Mejias2012,
author = {Mejias, J F and Longtin, A},
file = {:C$\backslash$:/Users/yann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mejias, Longtin - 2012 - Optimal heterogeneity for coding in spiking neural networks.pdf:pdf},
journal = {Physical Review Letters},
mendeley-groups = {Neuroscience},
number = {22},
pages = {228102},
publisher = {APS},
title = {{Optimal heterogeneity for coding in spiking neural networks}},
volume = {108},
year = {2012}
}

@article{Brette2005a,
author = {Brette, Romain and Gerstner, Wulfram},
doi = {10.1152/jn.00686.2005.},
file = {:C$\backslash$:/Users/yann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Brette, Gerstner - 2005 - Adaptive exponential integrate-and-fire model as an effective description of neuronal activity.pdf:pdf},
journal = {Journal of neurophysiology},
mendeley-groups = {Neuroscience,Neuroscience/Neuronal models},
pages = {3637--3642},
title = {{Adaptive exponential integrate-and-fire model as an effective description of neuronal activity}},
url = {http://jn.physiology.org/content/94/5/3637.short},
year = {2005}
}

@article{Rauch2003,
abstract = {In the intact brain neurons are constantly exposed to intense synaptic activity. This heavy barrage of excitatory and inhibitory inputs was recreated in vitro by injecting a noisy current, generated as an Ornstein-Uhlenbeck process, into the soma of rat neocortical pyramidal cells. The response to such in vivo-like currents was studied systematically by analyzing the time development of the instantaneous spike frequency, and when possible, the stationary mean spike frequency as a function of both the mean and the variance of the input current. All cells responded with an in vivo-like action potential activity with stationary statistics that could be sustained throughout long stimulation intervals (tens of seconds), provided the frequencies were not too high. The temporal evolution of the response revealed the presence of mechanisms of fast and slow spike frequency adaptation, and a medium duration mechanism of facilitation. For strong input currents, the slow adaptation mechanism made the spike frequency response nonstationary. The minimal frequencies that caused strong slow adaptation (a decrease in the spike rate by more than 1 Hz/s), were in the range 30-80 Hz and depended on the pipette solution used. The stationary response function has been fitted by two simple models of integrate-and-fire neurons endowed with a frequency-dependent modification of the input current. This accounts for all the fast and slow mechanisms of adaptation and facilitation that determine the stationary response, and proved necessary to fit the model to the experimental data. The coefficient of variability of the interspike interval was also in part captured by the model neurons, by tuning the parameters of the model to match the mean spike frequencies only. We conclude that the integrate-and-fire model with spike-frequency-dependent adaptation/facilitation is an adequate model reduction of cortical cells when the mean spike-frequency response to in vivo-like currents with stationary statistics is considered.},
author = {Rauch, Alexander and {La Camera}, Giancarlo and L\"{u}scher, Hans-Rudolf and Senn, Walter and Fusi, Stefano and Luscher, Hans-Rudolf},
doi = {10.1152/jn.00293.2003},
file = {:C$\backslash$:/Users/yann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rauch et al. - 2003 - Neocortical pyramidal cells respond as integrate-and-fire neurons to in vivo-like input currents.pdf:pdf},
issn = {0022-3077},
journal = {J Neurophysiol},
keywords = {Action Potentials,Action Potentials: physiology,Animals,Biological,Female,Male,Models,Neocortex,Neocortex: physiology,Neurons,Neurons: physiology,Pyramidal Cells,Pyramidal Cells: physiology,Rats,Wistar},
mendeley-groups = {Neuroscience},
month = sep,
number = {3},
pages = {1598--1612},
pmid = {12750422},
title = {{Neocortical Pyramidal Cells Respond as Integrate-and-Fire Neurons to In Vivo-Like Input Currents}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/12750422},
volume = {90},
year = {2003}
}

@article{Foust2010,
author = {Foust, A and Popovic, M},
doi = {10.1523/JNEUROSCI.0552-10.2010.Action},
file = {:C$\backslash$:/Users/yann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Foust, Popovic - 2010 - Action potentials initiate in the axon initial segment and propagate through axon collaterals reliably in cerebe.pdf:pdf},
journal = {The Journal of \ldots},
keywords = {action potential,axon,conduction failure,initiation,purkinje},
mendeley-groups = {Neuroscience},
number = {20},
pages = {6891--6902},
title = {{Action potentials initiate in the axon initial segment and propagate through axon collaterals reliably in cerebellar Purkinje neurons}},
url = {http://www.jneurosci.org/content/30/20/6891.short},
volume = {30},
year = {2010}
}

@article{Platkiewicz2010,
abstract = {In central neurons, the threshold for spike initiation can depend on the stimulus and varies between cells and between recording sites in a given cell, but it is unclear what mechanisms underlie this variability. Properties of ionic channels are likely to play a role in threshold modulation. We examined in models the influence of Na channel activation, inactivation, slow voltage-gated channels and synaptic conductances on spike threshold. We propose a threshold equation which quantifies the contribution of all these mechanisms. It provides an instantaneous time-varying value of the threshold, which applies to neurons with fluctuating inputs. We deduce a differential equation for the threshold, similar to the equations of gating variables in the Hodgkin-Huxley formalism, which describes how the spike threshold varies with the membrane potential, depending on channel properties. We find that spike threshold depends logarithmically on Na channel density, and that Na channel inactivation and K channels can dynamically modulate it in an adaptive way: the threshold increases with membrane potential and after every action potential. Our equation was validated with simulations of a previously published multicompartemental model of spike initiation. Finally, we observed that threshold variability in models depends crucially on the shape of the Na activation function near spike initiation (about -55 mV), while its parameters are adjusted near half-activation voltage (about -30 mV), which might explain why many models exhibit little threshold variability, contrary to experimental observations. We conclude that ionic channels can account for large variations in spike threshold.},
author = {Platkiewicz, Jonathan and Brette, Romain},
doi = {10.1371/journal.pcbi.1000850},
file = {:C$\backslash$:/Users/yann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Platkiewicz, Brette - 2010 - A threshold equation for action potential initiation.pdf:pdf},
issn = {1553-7358},
journal = {PLoS computational biology},
keywords = {Action Potentials,Action Potentials: physiology,Algorithms,Computer Simulation,Models, Neurological,Reproducibility of Results,Sodium Channels,Sodium Channels: physiology},
mendeley-groups = {Neuroscience,Neuroscience/Neuronal models},
month = jan,
number = {7},
pages = {e1000850},
pmid = {20628619},
title = {{A threshold equation for action potential initiation.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2900290\&tool=pmcentrez\&rendertype=abstract},
volume = {6},
year = {2010}
}

@incollection{Brette2005,
author = {Brette, Romain and Destexhe, Alain},
file = {:C$\backslash$:/Users/yann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Brette, Destexhe - 2005 - Intracellular recording.pdf:pdf},
mendeley-groups = {Neuroscience,Neuroscience/in\_vitro\_\mathrm{m}ethods},
title = {{Intracellular recording}},
year = {2005}
}

@article{Brette2013,
abstract = {In cortical neurons, spikes are initiated in the axon initial segment. Seen at the soma, they appear surprisingly sharp. A standard explanation is that the current coming from the axon becomes sharp as the spike is actively backpropagated to the soma. However, sharp initiation of spikes is also seen in the input-output properties of neurons, and not only in the somatic shape of spikes; for example, cortical neurons can transmit high frequency signals. An alternative hypothesis is that Na channels cooperate, but it is not currently supported by direct experimental evidence. I propose a simple explanation based on the compartmentalization of spike initiation. When Na channels are placed in the axon, the soma acts as a current sink for the Na current. I show that there is a critical distance to the soma above which an instability occurs, so that Na channels open abruptly rather than gradually as a function of somatic voltage.},
author = {Brette, Romain},
doi = {10.1371/journal.pcbi.1003338},
file = {:C$\backslash$:/Users/yann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Brette - 2013 - Sharpness of spike initiation in neurons explained by compartmentalization(2).pdf:pdf},
issn = {1553-7358},
journal = {PLoS computational biology},
mendeley-groups = {Neuroscience/Cell properties},
month = dec,
number = {12},
pages = {e1003338},
pmid = {24339755},
title = {{Sharpness of spike initiation in neurons explained by compartmentalization.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3854010\&tool=pmcentrez\&rendertype=abstract},
volume = {9},
year = {2013}
}


@book{Tuckwell1998b,
author = {Tuckwell, Henry C},
mendeley-groups = {Neuroscience},
publisher = {Cambridge University Press},
title = {{Introduction to theoretical neurobiology: Volume 2, nonlinear and stochastic theories}},
volume = {8},
year = {1988}
}

@article{Destexhe2009a,
abstract = {Randomly-connected networks of integrate-and-fire (IF) neurons are known to display asynchronous irregular (AI) activity states, which resemble the discharge activity recorded in the cerebral cortex of awake animals. However, it is not clear whether such activity states are specific to simple IF models, or if they also exist in networks where neurons are endowed with complex intrinsic properties similar to electrophysiological measurements. Here, we investigate the occurrence of AI states in networks of nonlinear IF neurons, such as the adaptive exponential IF (Brette-Gerstner-Izhikevich) model. This model can display intrinsic properties such as low-threshold spike (LTS), regular spiking (RS) or fast-spiking (FS). We successively investigate the oscillatory and AI dynamics of thalamic, cortical and thalamocortical networks using such models. AI states can be found in each case, sometimes with surprisingly small network size of the order of a few tens of neurons. We show that the presence of LTS neurons in cortex or in thalamus, explains the robust emergence of AI states for relatively small network sizes. Finally, we investigate the role of spike-frequency adaptation (SFA). In cortical networks with strong SFA in RS cells, the AI state is transient, but when SFA is reduced, AI states can be self-sustained for long times. In thalamocortical networks, AI states are found when the cortex is itself in an AI state, but with strong SFA, the thalamocortical network displays Up and Down state transitions, similar to intracellular recordings during slow-wave sleep or anesthesia. Self-sustained Up and Down states could also be generated by two-layer cortical networks with LTS cells. These models suggest that intrinsic properties such as adaptation and low-threshold bursting activity are crucial for the genesis and control of AI states in thalamocortical networks.},
author = {Destexhe, Alain},
doi = {10.1007/s10827-009-0164-4},
file = {:C$\backslash$:/Users/yann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Destexhe - 2009 - Self-sustained asynchronous irregular states and Up-Down states in thalamic, cortical and thalamocortical networks of.pdf:pdf},
issn = {1573-6873},
journal = {Journal of computational neuroscience},
keywords = {Action Potentials,Action Potentials: physiology,Animals,Biophysics,Cerebral Cortex,Cerebral Cortex: cytology,Cerebral Cortex: physiology,Electric Stimulation,Electric Stimulation: Methods,Excitatory Amino Acid Agonists,Excitatory Amino Acid Agonists: pharmacology,Models, Neurological,Nerve Net,Nerve Net: physiology,Neural Inhibition,Neural Inhibition: physiology,Neural Networks (Computer),Neural Pathways,Neural Pathways: physiology,Neurons,Neurons: classification,Neurons: physiology,Nonlinear Dynamics,Periodicity,Synaptic Transmission,Synaptic Transmission: drug effects,Synaptic Transmission: physiology,Thalamus,Thalamus: cytology,Thalamus: physiology,Wakefulness,alpha-Amino-3-hydroxy-5-methyl-4-isoxazolepropioni,gamma-Aminobutyric Acid,gamma-Aminobutyric Acid: pharmacology},
mendeley-groups = {Neuroscience},
month = dec,
number = {3},
pages = {493--506},
pmid = {19499317},
title = {{Self-sustained asynchronous irregular states and Up-Down states in thalamic, cortical and thalamocortical networks of nonlinear integrate-and-fire neurons.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/19499317},
volume = {27},
year = {2009}
}

@article{ElBoustani2009,
abstract = {Many efforts have been devoted to modeling asynchronous irregular (AI) activity states, which resemble the complex activity states seen in the cerebral cortex of awake animals. Most of models have considered balanced networks of excitatory and inhibitory spiking neurons in which AI states are sustained through recurrent sparse connectivity, with or without external input. In this letter we propose a mesoscopic description of such AI states. Using master equation formalism, we derive a second-order mean-field set of ordinary differential equations describing the temporal evolution of randomly connected balanced networks. This formalism takes into account finite size effects and is applicable to any neuron model as long as its transfer function can be characterized. We compare the predictions of this approach with numerical simulations for different network configurations and parameter spaces. Considering the randomly connected network as a unit, this approach could be used to build large-scale networks of such connected units, with an aim to model activity states constrained by macroscopic measurements, such as voltage-sensitive dye imaging.},
author = {{El Boustani}, Sami and Destexhe, Alain},
doi = {10.1162/neco.2009.02-08-710},
file = {:C$\backslash$:/Users/yann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/El Boustani, Destexhe - 2009 - A master equation formalism for macroscopic modeling of asynchronous irregular activity states.pdf:pdf},
issn = {0899-7667},
journal = {Neural computation},
keywords = {Action Potentials,Action Potentials: physiology,Animals,Cerebral Cortex,Cerebral Cortex: cytology,Cerebral Cortex: physiology,Computer Simulation,Linear Models,Models,Neural Inhibition,Neural Inhibition: physiology,Neural Networks (Computer),Neurological,Neurons,Neurons: classification,Neurons: physiology,Time Factors,Wakefulness},
mendeley-groups = {Neuroscience/Network Dynamics},
month = jan,
number = {1},
pages = {46--100},
pmid = {19210171},
publisher = {MIT Press},
title = {{A master equation formalism for macroscopic modeling of asynchronous irregular activity states}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/19210171},
volume = {21},
year = {2009}
}

@book{Ricciardi1977,
author = {Ricciardi, Luigi M},
mendeley-groups = {Neuroscience},
publisher = {Springer-Verlag Berlin},
title = {{Diffusion processes and related topics in biology}},
year = {1977}
}

@article{Kumar2008,
abstract = {We studied the dynamics of large networks of spiking neurons with conductance-based (nonlinear) synapses and compared them to networks with current-based (linear) synapses. For systems with sparse and inhibition-dominated recurrent connectivity, weak external inputs induced asynchronous irregular firing at low rates. Membrane potentials fluctuated a few millivolts below threshold, and membrane conductances were increased by a factor 2 to 5 with respect to the resting state. This combination of parameters characterizes the ongoing spiking activity typically recorded in the cortex in vivo. Many aspects of the asynchronous irregular state in conductance-based networks could be sufficiently well characterized with a simple numerical mean field approach. In particular, it correctly predicted an intriguing property of conductance-based networks that does not appear to be shared by current-based models: they exhibit states of low-rate asynchronous irregular activity that persist for some period of time even in the absence of external inputs and without cortical pacemakers. Simulations of larger networks (up to 350,000 neurons) demonstrated that the survival time of self-sustained activity increases exponentially with network size.},
author = {Kumar, Arvind and Schrader, Sven and Aertsen, Ad and Rotter, Stefan},
doi = {10.1162/neco.2008.20.1.1},
file = {:C$\backslash$:/Users/yann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kumar et al. - 2008 - The high-conductance state of cortical networks.pdf:pdf},
issn = {0899-7667},
journal = {Neural Computation},
keywords = {Action Potentials,Action Potentials: physiology,Animals,Cell Membrane,Cell Membrane: physiology,Cerebral Cortex,Cerebral Cortex: physiology,Computer Simulation,Cortical Synchronization,Excitatory Postsynaptic Potentials,Excitatory Postsynaptic Potentials: physiology,Humans,Inhibitory Postsynaptic Potentials,Inhibitory Postsynaptic Potentials: physiology,Nerve Net,Nerve Net: physiology,Neural Networks (Computer),Neural Pathways,Neural Pathways: physiology,Neurons,Neurons: physiology,Nonlinear Dynamics,Synaptic Transmission,Synaptic Transmission: physiology,Time Factors},
mendeley-groups = {Neuroscience},
month = jan,
number = {1},
pages = {1--43},
pmid = {18044999},
publisher = {MIT Press},
title = {{The high-conductance state of cortical networks}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/18044999},
volume = {20},
year = {2008}
}

@article{Richardson2004,
author = {Richardson, Magnus J E},
file = {:C$\backslash$:/Users/yann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Richardson - 2004 - Effects of synaptic conductance on the voltage distribution and firing rate of spiking neurons.pdf:pdf},
journal = {Physical Review E},
number = {5},
pages = {51918},
publisher = {APS},
title = {{Effects of synaptic conductance on the voltage distribution and firing rate of spiking neurons}},
volume = {69},
year = {2004}
}

@book{Tuckwell1988b,
author = {Tuckwell, Henry C},
mendeley-groups = {Neuroscience},
publisher = {Cambridge University Press},
title = {{Introduction to theoretical neurobiology: Volume 2, nonlinear and stochastic theories}},
volume = {8},
year = {1988}
}
@article{Amit1997,
abstract = {We investigate self-sustaining stable states (attractors) in networks of integrate-and-fire neurons. First, we study the stability of spontaneous activity in an unstructured network. It is shown that the stochastic background activity, of 1-5 spikes/s, is unstable if all neurons are excitatory. On the other hand, spontaneous activity becomes self-stabilizing in presence of local inhibition, given reasonable values of the parameters of the network. Second, in a network sustaining physiological spontaneous rates, we study the effect of learning in a local module, expressed in synaptic modifications in specific populations of synapses. We find that if the average synaptic potentiation (LTP) is too low, no stimulus specific activity manifests itself in the delay period. Instead, following the presentation and removal of any stimulus there is, in the local module, a delay activity in which all neurons selective (responding visually) to any of the stimuli presented for learning have rates which gradually increase with the amplitude of synaptic potentiation. When the average LTP increases beyond a critical value, specific local attractors (stable states) appear abruptly against the background of the global uniform spontaneous attractor. In this case the local module has two available types of collective delay activity: if the stimulus is unfamiliar, the activity is spontaneous; if it is similar to a learned stimulus, delay activity is selective. These new attractors reflect the synaptic structure developed during learning. In each of them a small population of neurons have elevated rates, which depend on the strength of LTP. The remaining neurons of the module have their activity at spontaneous rates. The predictions made in this paper could be checked by single unit recordings in delayed response experiments.},
author = {Amit, Daniel J. and Brunel, Nicolas},
file = {:home/yann/Documents/Mendeley//Amit, Brunel - 1997 - Model of global spontaneous activity and local structured activity during delay periods in the cerebral cortex.pdf:pdf},
issn = {1047-3211},
journal = {Cerebral Cortex},
keywords = {Afferent,Afferent: physiology,Cerebral Cortex,Cerebral Cortex: cytology,Cerebral Cortex: physiology,Electrophysiology,Feedback,Feedback: physiology,Long-Term Potentiation,Long-Term Potentiation: physiology,Models,Motor Activity,Motor Activity: physiology,Neural Networks (Computer),Neurological,Neurons,Neurons: physiology,Poisson Distribution,Synaptic Membranes,Synaptic Membranes: physiology},
mendeley-groups = {Neuroscience},
number = {3},
pages = {237--252},
pmid = {9143444},
publisher = {Oxford Univ Press},
title = {{Model of global spontaneous activity and local structured activity during delay periods in the cerebral cortex.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/9143444},
volume = {7},
year = {1997}
}


@article{LaCamera2008,
abstract = {The study of several aspects of the collective dynamics of interacting neurons can be highly simplified if one assumes that the statistics of the synaptic input is the same for a large population of similarly behaving neurons (mean field approach). In particular, under such an assumption, it is possible to determine and study all the equilibrium points of the network dynamics when the neuronal response to noisy, in vivo-like, synaptic currents is known. The response function can be computed analytically for simple integrate-and-fire neuron models and it can be measured directly in experiments in vitro. Here we review theoretical and experimental results about the neural response to noisy inputs with stationary statistics. These response functions are important to characterize the collective neural dynamics that are proposed to be the neural substrate of working memory, decision making and other cognitive functions. Applications to the case of time-varying inputs are reviewed in a companion paper (Giugliano et al. in Biol Cybern, 2008). We conclude that modified integrate-and-fire neuron models are good enough to reproduce faithfully many of the relevant dynamical aspects of the neuronal response measured in experiments on real neurons in vitro.},
author = {{La Camera}, Giancarlo and Giugliano, Michele and Senn, Walter and Fusi, Stefano},
doi = {10.1007/s00422-008-0272-7},
file = {:C$\backslash$:/Users/yann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/La Camera et al. - 2008 - The response of cortical neurons to in vivo-like input current theory and experiment I. Noisy inputs with sta.pdf:pdf},
issn = {1432-0770},
journal = {Biological cybernetics},
keywords = {Animals,Cerebral Cortex,Cerebral Cortex: physiology,Humans,Models,Neurological,Neurons,Neurons: physiology},
mendeley-groups = {Neuroscience},
month = nov,
number = {4-5},
pages = {279--301},
pmid = {18985378},
title = {{The response of cortical neurons to in vivo-like input current: theory and experiment : I. Noisy inputs with stationary statistics.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/18985378},
volume = {99},
year = {2008}
}

@article{McCormick1985,
abstract = {Slices of sensorimotor and anterior cingulate cortex from guinea pigs were maintained in vitro and bathed in a normal physiological medium. Electrophysiological properties of neurons were assessed with intracellular recording techniques. Some neurons were identified morphologically by intracellular injection of the fluorescent dye Lucifer yellow CH. Three distinct neuronal classes of electrophysiological behavior were observed; these were termed regular spiking, bursting, and fast spiking. The physiological properties of neurons from sensorimotor and anterior cingulate areas did not differ significantly. Regular-spiking cells were characterized by action potentials with a mean duration of 0.80 ms at one-half amplitude, a ratio of maximum rate of spike rise to maximum rate of fall of 4.12, and a prominent afterhyperpolarization following a train of spikes. The primary slope of initial spike frequency versus injected current intensity was 241 Hz/nA. During prolonged suprathreshold current pulses the frequency of firing adapted strongly. When local synaptic pathways were activated, all cells were transiently excited and then strongly inhibited. Bursting cells were distinguished by their ability to generate endogenous, all-or-none bursts of three to five action potentials. Their properties were otherwise very similar to regular-spiking cells. The ability to generate a burst was eliminated when the membrane was depolarized to near the firing threshold with tonic current. By contrast, hyperpolarization of regular-spiking (i.e., nonbursting) cells did not uncover latent bursting tendencies. The action potentials of fast-spiking cells were much briefer (mean of 0.32 ms) than those of the other cell types.(ABSTRACT TRUNCATED AT 250 WORDS)},
author = {McCormick, David A and Connors, B W and Lighthall, J W and Prince, D a},
file = {:home/yann/Documents/Mendeley//McCormick et al.\_1985\_Comparative electrophysiology of pyramidal and sparsely spiny stellate neurons of the neocortex.pdf:pdf},
issn = {0022-3077},
journal = {Journal of neurophysiology},
keywords = {Action Potentials,Animals,Cerebral Cortex,Cerebral Cortex: cytology,Cerebral Cortex: physiology,Electrophysiology,Glutamate Decarboxylase,Glutamate Decarboxylase: metabolism,Guinea Pigs,Gyrus Cinguli,Gyrus Cinguli: physiology,Motor Cortex,Motor Cortex: physiology,Somatosensory Cortex,Somatosensory Cortex: physiology,Synaptic Transmission,gamma-Aminobutyric Acid,gamma-Aminobutyric Acid: physiology},
mendeley-groups = {Neuroscience},
month = oct,
number = {4},
pages = {782--806},
pmid = {2999347},
title = {{Comparative electrophysiology of pyramidal and sparsely spiny stellate neurons of the neocortex.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/2999347},
volume = {54},
year = {1985}
}

@article{McCormick2007,
abstract = {Action potentials in cortical neurons show a variable threshold and a sudden rise in membrane potential at initiation. Naundorf et al. fail to explain these features using single- or double-compartment Hodgkin-Huxley-style models, suggesting instead that they could arise from cooperative opening of Na+ channels, although there is no direct biological evidence to support this. Here we show that these so-called unique features are to be expected from Hodgkin-Huxley models if the spatial geometry and spike initiation properties of cortical neurons are taken into account--it is therefore unnecessary to invoke exotic channel-gating properties as an explanation.},
author = {McCormick, David a and Shu, Yousheng and Yu, Yuguo},
doi = {10.1038/nature05523},
file = {:home/yann/Documents/Mendeley/McCormick, Shu, Yu\_2007\_Neurophysiology Hodgkin and Huxley model--still standing.pdf:pdf},
issn = {1476-4687},
journal = {Nature},
keywords = {Action Potentials,Axons,Axons: metabolism,Electrophysiology,Models,Neurological,Pyramidal Cells,Pyramidal Cells: cytology,Pyramidal Cells: metabolism,Sodium Channels,Sodium Channels: metabolism},
mendeley-groups = {Neuroscience},
month = jan,
number = {7123},
pages = {E1--2; discussion E2--3},
pmid = {17203021},
title = {{Neurophysiology: Hodgkin and Huxley model--still standing?}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17203021},
volume = {445},
year = {2007}
}

@article{Naundorf2006,
abstract = {Neurons process and encode information by generating sequences of action potentials. For all spiking neurons, the encoding of single-neuron computations into sequences of spikes is biophysically determined by the cell's action-potential-generating mechanism. It has recently been discovered that apparently minor modifications of this mechanism can qualitatively change the nature of neuronal encoding. Here we quantitatively analyse the dynamics of action potential initiation in cortical neurons in vivo, in vitro and in computational models. Unexpectedly, key features of the initiation dynamics of cortical neuron action potentials--their rapid initiation and variable onset potential--are outside the range of behaviours described by the classical Hodgkin-Huxley theory. We propose a new model based on the cooperative activation of sodium channels that reproduces the observed dynamics of action potential initiation. This new model predicts that Hodgkin-Huxley-type dynamics of action potential initiation can be induced by artificially decreasing the effective density of sodium channels. In vitro experiments confirm this prediction, supporting the hypothesis that cooperative sodium channel activation underlies the dynamics of action potential initiation in cortical neurons.},
author = {Naundorf, Bj\"{o}rn and Wolf, Fred and Volgushev, Maxim},
doi = {10.1038/nature04610},
file = {:home/yann/Documents/Mendeley/Naundorf, Wolf, Volgushev\_2006\_Unique features of action potential initiation in cortical neurons.pdf:pdf},
issn = {1476-4687},
journal = {Nature},
keywords = {Action Potentials,Action Potentials: physiology,Animals,Cats,Cerebral Cortex,Cerebral Cortex: cytology,Computer Simulation,Ion Channel Gating,Mice,Models, Neurological,Neocortex,Neocortex: cytology,Neurons,Neurons: physiology,Rats,Sodium Channels,Sodium Channels: metabolism,Time Factors,Visual Cortex,Visual Cortex: cytology},
mendeley-groups = {Neuroscience},
month = apr,
number = {7087},
pages = {1060--3},
pmid = {16625198},
title = {{Unique features of action potential initiation in cortical neurons.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/16625198},
volume = {440},
year = {2006}
}

@article{Badel2008,
abstract = {The dynamic I-V curve method was recently introduced for the efficient experimental generation of reduced neuron models. The method extracts the response properties of a neuron while it is subject to a naturalistic stimulus that mimics in vivo-like fluctuating synaptic drive. The resulting history-dependent, transmembrane current is then projected onto a one-dimensional current-voltage relation that provides the basis for a tractable non-linear integrate-and-fire model. An attractive feature of the method is that it can be used in spike-triggered mode to quantify the distinct patterns of post-spike refractoriness seen in different classes of cortical neuron. The method is first illustrated using a conductance-based model and is then applied experimentally to generate reduced models of cortical layer-5 pyramidal cells and interneurons, in injected-current and injected- conductance protocols. The resulting low-dimensional neuron models-of the refractory exponential integrate-and-fire type-provide highly accurate predictions for spike-times. The method therefore provides a useful tool for the construction of tractable models and rapid experimental classification of cortical neurons.},
author = {Badel, Laurent and Lefort, Sandrine and Berger, Thomas K. and Petersen, Carl C H and Gerstner, Wulfram and Richardson, Magnus J E},
doi = {10.1007/s00422-008-0259-4},
isbn = {1432-0770 (Electronic)$\backslash$n0340-1200 (Linking)},
issn = {03401200},
journal = {Biological Cybernetics},
keywords = {Exponential integrate-and-fire,I-V curve,Refractoriness},
pages = {361--370},
pmid = {19011924},
title = {{Extracting non-linear integrate-and-fire models from experimental data using dynamic I-V curves}},
volume = {99},
year = {2008}
}

@article{Vreeswijk1996,
abstract = {Neurons in the cortex of behaving animals show temporally irregular spiking patterns. The origin of this irregularity and its implications for neural processing are unknown. The hypothesis that the temporal variability in the firing of a neuron results from an approximate balance between its excitatory and inhibitory inputs was investigated theoretically. Such a balance emerges naturally in large networks of excitatory and inhibitory neuronal populations that are sparsely connected by relatively strong synapses. The resulting state is characterized by strongly chaotic dynamics, even when the external inputs to the network are constant in time. Such a network exhibits a linear response, despite the highly nonlinear dynamics of single neurons, and reacts to changing external stimuli on time scales much smaller than the integration time constant of a single neuron.},
author = {van Vreeswijk, Carl and Sompolinsky, Haim},
file = {:C$\backslash$:/Users/yann/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/van Vreeswijk, Sompolinsky - 1996 - Chaos in neuronal networks with balanced excitatory and inhibitory activity.pdf:pdf},
issn = {0036-8075},
journal = {Science},
keywords = {Animals,Cerebral Cortex,Cerebral Cortex: cytology,Cerebral Cortex: physiology,Haplorhini,Models,Nerve Net,Nerve Net: physiology,Neurological,Neurons,Neurons: physiology,Nonlinear Dynamics,Prefrontal Cortex,Prefrontal Cortex: physiology,Synapses,Synapses: physiology},
mendeley-groups = {Neuroscience},
month = dec,
number = {5293},
pages = {1724--1726},
pmid = {8939866},
publisher = {American Association for the Advancement of Science},
title = {{Chaos in neuronal networks with balanced excitatory and inhibitory activity}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/8939866},
volume = {274},
year = {1996}
}

@article{Destexhe1999,
abstract = {During wakefulness, neocortical neurons are subjected to an intense synaptic bombardment. To assess the consequences of this background activity for the integrative properties of pyramidal neurons, we constrained biophysical models with in vivo intracellular data obtained in anesthetized cats during periods of intense network activity similar to that observed in the waking state. In pyramidal cells of the parietal cortex (area 5-7), synaptic activity was responsible for an approximately fivefold decrease in input resistance (Rin), a more depolarized membrane potential (Vm), and a marked increase in the amplitude of Vm fluctuations, as determined by comparing the same cells before and after microperfusion of tetrodotoxin (TTX). The model was constrained by measurements of Rin, by the average value and standard deviation of the Vm measured from epochs of intense synaptic activity recorded with KAc or KCl-filled pipettes as well as the values measured in the same cells after TTX. To reproduce all experimental results, the simulated synaptic activity had to be of relatively high frequency (1-5 Hz) at excitatory and inhibitory synapses. In addition, synaptic inputs had to be significantly correlated (correlation coefficient approximately 0.1) to reproduce the amplitude of Vm fluctuations recorded experimentally. The presence of voltage-dependent K+ currents, estimated from current-voltage relations after TTX, affected these parameters by <10\%. The model predicts that the conductance due to synaptic activity is 7-30 times larger than the somatic leak conductance to be consistent with the approximately fivefold change in Rin. The impact of this massive increase in conductance on dendritic attenuation was investigated for passive neurons and neurons with voltage-dependent Na+/K+ currents in soma and dendrites. In passive neurons, correlated synaptic bombardment had a major influence on dendritic attenuation. The electrotonic attenuation of simulated synaptic inputs was enhanced greatly in the presence of synaptic bombardment, with distal synapses having minimal effects at the soma. Similarly, in the presence of dendritic voltage-dependent currents, the convergence of hundreds of synaptic inputs was required to evoke action potentials reliably. In this case, however, dendritic voltage-dependent currents minimized the variability due to input location, with distal apical synapses being as effective as synapses on basal dendrites. In conclusion, this combination of intracellular and computational data suggests that, during low-amplitude fast electroencephalographic activity, neocortical neurons are bombarded continuously by correlated synaptic inputs at high frequency, which significantly affect their integrative properties. A series of predictions are suggested to test this model.},
author = {Destexhe, Alain and Par\'{e}, Denis},
file = {:home/yann/Documents/Mendeley/Destexhe, Par\'{e} - 1999 - Impact of network activity on the integrative properties of neocortical pyramidal neurons in vivo.pdf:pdf},
issn = {0022-3077},
journal = {Journal of neurophysiology},
keywords = {Action Potentials,Action Potentials: drug effects,Action Potentials: physiology,Adrenergic alpha-Agonists,Adrenergic alpha-Agonists: pharmacology,Animals,Awareness,Awareness: physiology,Cats,Dendrites,Dendrites: drug effects,Dendrites: physiology,Electric Conductivity,Electroencephalography,Excitatory Amino Acid Antagonists,Excitatory Amino Acid Antagonists: pharmacology,Excitatory Postsynaptic Potentials,Excitatory Postsynaptic Potentials: drug effects,Excitatory Postsynaptic Potentials: physiology,Ketamine,Ketamine: pharmacology,Neocortex,Neocortex: cytology,Neocortex: physiology,Nerve Net,Neural Pathways,Pyramidal Cells,Pyramidal Cells: physiology,Synapses,Synapses: drug effects,Synapses: physiology,Tetrodotoxin,Tetrodotoxin: pharmacology,Xylazine,Xylazine: pharmacology},
mendeley-groups = {Neuroscience},
month = apr,
number = {4},
pages = {1531--1547},
pmid = {10200189},
publisher = {Am Physiological Soc},
title = {{Impact of network activity on the integrative properties of neocortical pyramidal neurons in vivo}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/10200189},
volume = {81},
year = {1999}
}

@article{Destexhe2003,
author = {Destexhe, Alain and Rudolph, Michael and Par\'{e}, Denis},
file = {:home/yann/Documents/Mendeley//Destexhe, Rudolph, Par\'{e} - 2003 - The high-conductance state of neocortical neurons in vivo.pdf:pdf},
journal = {Nature Reviews Neuroscience},
mendeley-groups = {Neuroscience},
number = {9},
pages = {739--751},
publisher = {Nature Publishing Group},
title = {{The high-conductance state of neocortical neurons in vivo}},
volume = {4},
year = {2003}
}

@article{Renart2004,
author = {Renart, Alfonso and Brunel, Nicolas and Wang, Xiao-Jing},
file = {:home/yann/Documents/Mendeley//Renart, Brunel, Wang - 2004 - Mean-field theory of irregularly spiking neuronal populations and working memory in recurrent cortical net.pdf:pdf},
isbn = {1584883626},
journal = {Computational neuroscience: A comprehensive approach},
mendeley-groups = {Neuroscience},
pages = {431--490},
publisher = {Boca Raton, CRC Press},
title = {{Mean-field theory of irregularly spiking neuronal populations and working memory in recurrent cortical networks}},
year = {2004}
}

\end{filecontents}
#+END_LATEX

\bibliographystyle{apalike}
\bibliography{tex/biblio}

\onecolumn
\newpage
# \twocolumn


* Building Multi-Panel Figures :noexport:

** Making the panel of the experimental characterization figure

**** PANEL
#+begin_src python
import svgutils.transform as sg
fig = sg.SVGFigure("14cm", "9.5cm")

# load matpotlib-generated figures
fig1 = sg.fromfile('../figures/cell_biocytin.svg')
fig2 = sg.fromfile('../figures/full_exp.svg')
fig3 = sg.fromfile('../figures/cell_trace.svg')
fig4 = sg.fromfile('../figures/recording_length.svg')
fig5 = sg.fromfile('../figures/vc_protocol.svg')
fig6 = sg.fromfile('../figures/fi_curve.svg')
fig7 = sg.fromfile('../figures/fi_curve_traces.svg')


# get the plot objects
plot1 = fig1.getroot();plot1.moveto(16, 2, scale=0.04)
plot2 = fig2.getroot();plot2.moveto(250, 5, scale=.9)
plot3 = fig3.getroot();plot3.moveto(0, 170, scale=1)
plot4 = fig4.getroot();plot4.moveto(165, 90, scale=.76)
plot5 = fig5.getroot();plot5.moveto(66, 95, scale=.6)
plot6 = fig6.getroot();plot6.moveto(70, 10, scale=.55)
plot7 = fig7.getroot();plot7.moveto(170, 0, scale=.28)

# add text labels
txt1 = sg.TextElement(0,15, "A", size=14, weight="bold")
txt2 = sg.TextElement(240,15, "E", size=14, weight="bold")
txt3 = sg.TextElement(0,185, "F", size=14, weight="bold")
txt4 = sg.TextElement(65,95, "C", size=14, weight="bold")
txt5 = sg.TextElement(160,95, "D", size=14, weight="bold")
txt6 = sg.TextElement(65,15, "B", size=14, weight="bold")

# append plots and labels to figure
fig.append([plot1, plot3, plot5, plot6, plot7, plot2, plot4])
fig.append([txt1, txt2, txt3, txt4, txt5, txt6])

# save generated SVG files
fig.save("fig1.svg")

import os
os.system('inkscape --export-pdf=fig1.pdf fig1.svg')
os.system('eog fig1.svg')
# os.system('rm fig1.svg')
#+end_src

#+RESULTS:
: None

     
**** CELL ACCESS FIGURE
#+begin_src python
import sys
import matplotlib.pylab as plt
import numpy as np
sys.path.append('../experimental_data/')
from first_paper_fig import fancy_VC_membrane_test
DATAFILE = '/media/yann/DATA_EqAlain/files/DATA/2015_2_17/21_24_40_VC_MEMBRANETEST1.DAT'
fig1 = fancy_VC_membrane_test(DATAFILE,FIGSIZE=(2,1.5), FONTSIZE=12)
plt.show()
fig1.savefig('../figures/vc_protocol.svg', format='svg', transparent=True)
#+end_src

#+RESULTS:
: None


**** MEMBRANE PROP FIGURE
#+begin_src python
# fig2 = funcs.fancy_IC_membrane_test(cell_params['ROOT_FOLDER']+cell_params['IC_datafile'],\
#                                     FIGSIZE=(6,3.6))
# fig2.savefig('../figures/ic_protocol.svg', format='svg', transparent=True)
import svgutils.transform as sg
import sys 
fig = sg.SVGFigure("18cm", "15cm")
# load matpotlib-generated figures
fig1 = sg.fromfile('../figures/cell_biocytin.svg')
fig2 = sg.fromfile('../figures/full_exp.svg')
fig3 = sg.fromfile('../figures/cell_trace.svg')

# get the plot objects
plot1 = fig1.getroot()
plot1.moveto(20, 20, scale=0.1)
plot2 = fig2.getroot()
plot2.moveto(280, 0, scale=0.5)
plot3 = fig3.getroot()
plot3.moveto(10, 0, scale=0.5)

# add text labels
txt1 = sg.TextElement(25,20, "A", size=12, weight="bold")
txt2 = sg.TextElement(305,20, "B", size=12, weight="bold")

# append plots and labels to figure
fig.append([plot1, plot2])
fig.append([txt1, txt2])

# save generated SVG files
fig.save("fig1.svg")

import os
os.system('eog fig1.svg')
# os.system('inkscape --export-pdf=fig1.pdf fig1.svg')
# os.system('rm fig1.svg')
#+end_src
#+RESULTS:
: None

[[file:fig1.pdf]]


**** FI-CURVE FIGURE

***** REAL FI CURVE
#+begin_src python
FONTSIZE=16
import matplotlib
font = {'size'   : FONTSIZE}
matplotlib.rc('font', **font)
import sys
import matplotlib.pylab as plt
import numpy as np
sys.path.append('/home/yann/work/python_library/')
from my_graph import adjust_spines, cm2inch
import classic_electrophy as ce
DATAFILE = '/media/yann/DATA_EqAlain/files/DATA/2015_2_16/20_39_26_FI-CURVE1.DAT'
II, FF = ce.FI_curve_analysis(DATAFILE, crossing=-30, return_fig=False)
plt.figure(figsize=cm2inch(6,5))
plt.subplots_adjust(top=.95, right=.95,left=0.28, bottom=0.3)
ax = plt.subplot(111)
II.insert(0, 0)
FF.insert(0, 0)
plt.plot(II, FF, 'kD-', ms=4)
plt.plot([II[4], II[8]], [FF[4],FF[8]], 'ko', ms=7, lw=0)
plt.ylim([-1,32]);plt.ylabel('$\\nu_\mathrm{out}$ (Hz)')
plt.yticks([0,10,20,30])
plt.xlim([-10,210]);plt.xlabel('current (pA)')
plt.xticks([0,100,200])
adjust_spines(ax, ['left','bottom'])
plt.savefig('../figures/fi_curve.svg', format='svg', transparent=True)
#plt.show()
#+end_src

#+RESULTS:
: None


***** VM TRACES
#+begin_src python
FONTSIZE=24
import matplotlib
font = {'size'   : FONTSIZE}
matplotlib.rc('font', **font)
import sys
import matplotlib.pylab as plt
import numpy as np
sys.path.append('/home/yann/work/python_library/')
from my_graph import adjust_spines, cm2inch
import elphy_to_python as ep
DATAFILE = '/media/yann/DATA_EqAlain/files/DATA/2015_2_16/20_39_26_FI-CURVE1.DAT'
t, data = ep.get_signals_episode(DATAFILE)
i0, i1 = int(300/(t[1]-t[0])), int(1400/(t[1]-t[0]))
plt.figure(figsize=cm2inch(8,10))
plt.subplots_adjust(top=1, right=1,left=0, bottom=0)
ax = plt.subplot(111)
plt.plot(t[i0:i1], data[7][0][i0:i1], 'k-', lw=1)
plt.plot(t[i0:i1], data[3][0][i0:i1]-data[7][0].min()+10, 'k-', lw=1)
plt.plot([800,1000],[-50,-50], 'k-', lw=3)
plt.plot([800,800],[-50,-40], 'k-', lw=3)
plt.annotate('10mV', (460,-50), fontsize=FONTSIZE)
plt.annotate('200ms', (800,-65), fontsize=FONTSIZE)
plt.ylim([-70,110])
adjust_spines(ax, [])
plt.savefig('../figures/fi_curve_traces.svg', format='svg', transparent=True)
#plt.show()
#+end_src
#+RESULTS:
: None

[[file:fig1.pdf]]


**** VM TRACES AND MONITORING FIGURE
#+begin_src python
FONTSIZE = 10
import sys
import matplotlib.pylab as plt
sys.path.append('../experimental_data/')
from first_paper_fig import make_single_example_figs
# sizes of figures in cm !
fig3, fig4 = make_single_example_figs(FIG3 = [17,6], FIG4=[9,6.5], FONTSIZE = FONTSIZE)
plt.show()
fig3.savefig('../figures/cell_trace.svg', format='svg', transparent=True)
fig4.savefig('../figures/full_exp.svg', format='svg', transparent=True)
#+end_src

#+RESULTS:
: None


**** RECORDING LENGTH FIGURE
#+begin_src python
import sys
import matplotlib.pylab as plt
import numpy as np
sys.path.append('../experimental_data/')
from dataset_structure import make_recording_length_fig
data = np.load('../experimental_data/dataset.npy')
RS = data[5]
fig = make_recording_length_fig(RS, FIGSIZE=(4.4,3.5), FONTSIZE=10)
fig.subplots_adjust(bottom=0.3, left=0.24, top=0.95, right=0.96)
fig.savefig('../figures/recording_length.svg',format='svg',transparent=True)
plt.show()
#+end_src

#+RESULTS:
: None




*** Merged muG and Tv panel (model+exp)


*** (muV-sV) panel (model+exp)

**** PANEL
#+begin_src python
import svgutils.transform as sg
fig = sg.SVGFigure("16cm", "10cm")

# load matpotlib-generated figures
fig1 = sg.fromfile('../figures/muV_sV_firing.svg')
fig2 = sg.fromfile('../figures/muV_sV_subthreshold.svg')
fig3 = sg.fromfile('../figures/muV_sV_firing_exps.svg')
fig4 = sg.fromfile('../figures/muV_sV_subthreshold_exps.svg')
fig5 = sg.fromfile('../dependency_muV_sV/data/5.svg')
fig6 = sg.fromfile('../dependency_muV_sV/data/6.svg')
fig7 = sg.fromfile('../dependency_muV_sV/data/9.svg')

# get the plot objects
plot1 = fig1.getroot();plot1.moveto(2, 2, scale=.45)
plot2 = fig2.getroot();plot2.moveto(15, 140, scale=.4)
plot3 = fig3.getroot();plot3.moveto(210, 2, scale=.45)
plot4 = fig4.getroot();plot4.moveto(225, 140, scale=.4)
plot5 = fig5.getroot();plot5.moveto(350, 2, scale=.35)
plot6 = fig6.getroot();plot6.moveto(350, 110, scale=.35)
plot7 = fig7.getroot();plot7.moveto(350, 220, scale=.35)

# add text labels
txt1 = sg.TextElement(0,15, "A", size=14, weight="bold")
txt2 = sg.TextElement(240,15, "E", size=14, weight="bold")
txt3 = sg.TextElement(0,185, "F", size=14, weight="bold")
txt4 = sg.TextElement(65,95, "C", size=14, weight="bold")
txt5 = sg.TextElement(160,95, "D", size=14, weight="bold")
txt6 = sg.TextElement(65,15, "B", size=14, weight="bold")

# append plots and labels to figure
fig.append([plot1, plot3, plot5, plot6, plot7, plot2, plot4])
fig.append([txt1, txt2, txt3, txt4, txt5, txt6])

# save generated SVG files
fig.save("fig2.svg")


import os
os.system('inkscape --export-pdf=fig2.pdf fig2.svg')
os.system('eog fig2.svg')
# os.system('rm fig2.svg')
#+end_src

#+RESULTS:
: None




*** muG panel (model+exp)
    
**** PANEL
#+begin_src python
import sys
sys.path.append('/home/yann/work/transfer_functions/final_transfer_functions/muG_effect')
import experimental_data as exp
import main_paper_fig as th

import svgutils.transform as sg
import os
fig = sg.SVGFigure("10cm", "17cm")

# load matpotlib-generated figures
fig1 = sg.fromfile('../figures/muG_model_firing.svg')
fig2 = sg.fromfile('../figures/muG_model_subthre.svg')
fig3  = sg.fromfile('../figures/muG_data_full_spiking.svg')
fig4 = sg.fromfile('../figures/muG_data_full_subthre.svg')
fig5 = sg.fromfile('../figures/muG_data_full_vthre.svg')
fig6 = sg.fromfile('../figures/muG_data_full_hist.svg')

# get the plot objects
plot1 = fig1.getroot();plot1.moveto(2, 2, scale=.48)
plot2 = fig2.getroot();plot2.moveto(2, 115, scale=.48)
plot3 = fig3.getroot();plot3.moveto(140, 2, scale=.5)
plot4 = fig4.getroot();plot4.moveto(140, 140, scale=.5)
plot5 = fig5.getroot();plot5.moveto(140, 290, scale=.5)
plot6 = fig6.getroot();plot6.moveto(80, 400, scale=.5)

# add text labels
txt1 = sg.TextElement(0,15, "A", size=14, weight="bold")
txt2 = sg.TextElement(0,110, "B", size=14, weight="bold")
txt3 = sg.TextElement(145,15, "C", size=14, weight="bold")
txt4 = sg.TextElement(145,120, "D", size=14, weight="bold")
txt5 = sg.TextElement(145,295, "E", size=14, weight="bold")
txt6 = sg.TextElement(100,395, "F", size=14, weight="bold")

# append plots and labels to figure
fig.append([plot1, plot2, plot3, plot4, plot5, plot6])
fig.append([txt1, txt2, txt3, txt4, txt5, txt6])

# save generated SVG files
fig.save("fig3.svg")


# os.system('inkscape --export-pdf=fig3.pdf fig3.svg')
os.system('eog fig3.svg')
# os.system('rm fig2.svg')
#+end_src

#+RESULTS:
: None



     

*** Tv panel (model+exp)

**** PANEL
#+begin_src python
import sys
sys.path.append('/home/yann/work/transfer_functions/final_transfer_functions/muG_effect')
import experimental_data as exp
import main_paper_fig as th

import svgutils.transform as sg
import os
fig = sg.SVGFigure("10cm", "17cm")

# load matpotlib-generated figures
fig1 = sg.fromfile('../figures/Tv_model_firing.svg')
fig2 = sg.fromfile('../figures/Tv_model_subthre.svg')
fig3  = sg.fromfile('../figures/Tv_data_full_spiking.svg')
fig4 = sg.fromfile('../figures/Tv_data_full_subthre.svg')
fig5 = sg.fromfile('../figures/Tv_data_full_vthre.svg')
fig6 = sg.fromfile('../figures/Tv_data_full_hist.svg')

# get the plot objects
plot1 = fig1.getroot();plot1.moveto(2, 2, scale=.48)
plot2 = fig2.getroot();plot2.moveto(2, 115, scale=.48)
plot3 = fig3.getroot();plot3.moveto(140, 2, scale=.5)
plot4 = fig4.getroot();plot4.moveto(140, 140, scale=.5)
plot5 = fig5.getroot();plot5.moveto(140, 290, scale=.5)
plot6 = fig6.getroot();plot6.moveto(80, 400, scale=.5)

# add text labels
txt1 = sg.TextElement(0,15, "A", size=14, weight="bold")
txt2 = sg.TextElement(0,110, "B", size=14, weight="bold")
txt3 = sg.TextElement(145,15, "C", size=14, weight="bold")
txt4 = sg.TextElement(145,120, "D", size=14, weight="bold")
txt5 = sg.TextElement(145,295, "E", size=14, weight="bold")
txt6 = sg.TextElement(100,395, "F", size=14, weight="bold")

# append plots and labels to figure
fig.append([plot1, plot2, plot3, plot4, plot5, plot6])
fig.append([txt1, txt2, txt3, txt4, txt5, txt6])

# save generated SVG files
fig.save("fig4.svg")


# os.system('inkscape --export-pdf=fig4.pdf fig4.svg')
os.system('eog fig4.svg')
# os.system('rm fig4.svg')
#+end_src

#+RESULTS:
: None



     


*** muG-Tv panel (model+exp)

**** PANEL
#+begin_src python
import sys
import svgutils.transform as sg
import os
fig = sg.SVGFigure("10cm", "17cm")

# load matpotlib-generated figures
fig1 = sg.fromfile('../figures/muG_Tv_model_firing.svg')
fig2 = sg.fromfile('../figures/muG_Tv_model_subthre.svg')
fig3  = sg.fromfile('../figures/muG_Tv_data_full_spiking.svg')
fig4 = sg.fromfile('../figures/muG_Tv_data_full_subthre.svg')
fig5 = sg.fromfile('../figures/muG_Tv_data_full_vthre.svg')
fig6 = sg.fromfile('../figures/muG_Tv_data_full_hist.svg')

# get the plot objects
plot1 = fig1.getroot();plot1.moveto(2, 2, scale=.48)
plot2 = fig2.getroot();plot2.moveto(2, 115, scale=.48)
plot3 = fig3.getroot();plot3.moveto(140, 2, scale=.5)
plot4 = fig4.getroot();plot4.moveto(140, 140, scale=.5)
plot5 = fig5.getroot();plot5.moveto(140, 290, scale=.5)
plot6 = fig6.getroot();plot6.moveto(80, 400, scale=.5)

# add text labels
txt1 = sg.TextElement(0,15, "A", size=14, weight="bold")
txt2 = sg.TextElement(0,110, "B", size=14, weight="bold")
txt3 = sg.TextElement(145,15, "C", size=14, weight="bold")
txt4 = sg.TextElement(145,120, "D", size=14, weight="bold")
txt5 = sg.TextElement(145,295, "E", size=14, weight="bold")
txt6 = sg.TextElement(100,395, "F", size=14, weight="bold")

# append plots and labels to figure
fig.append([plot1, plot2, plot3, plot4, plot5, plot6])
fig.append([txt1, txt2, txt3, txt4, txt5, txt6])

# save generated SVG files
fig.save("fig5.svg")


# os.system('inkscape --export-pdf=fig5.pdf fig5.svg')
os.system('eog fig5.svg')
# os.system('rm fig5.svg')
#+end_src

#+RESULTS:
: None



     


*** 3D panel (model+exp)

**** PANEL
#+begin_src python
import sys
import svgutils.transform as sg
import os
fig = sg.SVGFigure("10cm", "17cm")

# load matpotlib-generated figures
fig1 = sg.fromfile('../figures/muG_Tv_model_firing.svg')
fig2 = sg.fromfile('../figures/muG_Tv_model_subthre.svg')
fig3  = sg.fromfile('../figures/muG_Tv_data_full_spiking.svg')
fig4 = sg.fromfile('../figures/muG_Tv_data_full_subthre.svg')
fig5 = sg.fromfile('../figures/muG_Tv_data_full_vthre.svg')
fig6 = sg.fromfile('../figures/muG_Tv_data_full_hist.svg')

# get the plot objects
plot1 = fig1.getroot();plot1.moveto(2, 2, scale=.48)
plot2 = fig2.getroot();plot2.moveto(2, 115, scale=.48)
plot3 = fig3.getroot();plot3.moveto(140, 2, scale=.5)
plot4 = fig4.getroot();plot4.moveto(140, 140, scale=.5)
plot5 = fig5.getroot();plot5.moveto(140, 290, scale=.5)
plot6 = fig6.getroot();plot6.moveto(80, 400, scale=.5)

# add text labels
txt1 = sg.TextElement(0,15, "A", size=14, weight="bold")
txt2 = sg.TextElement(0,110, "B", size=14, weight="bold")
txt3 = sg.TextElement(145,15, "C", size=14, weight="bold")
txt4 = sg.TextElement(145,120, "D", size=14, weight="bold")
txt5 = sg.TextElement(145,295, "E", size=14, weight="bold")
txt6 = sg.TextElement(100,395, "F", size=14, weight="bold")

# append plots and labels to figure
fig.append([plot1, plot2, plot3, plot4, plot5, plot6])
fig.append([txt1, txt2, txt3, txt4, txt5, txt6])

# save generated SVG files
fig.save("fig5.svg")


# os.system('inkscape --export-pdf=fig5.pdf fig5.svg')
os.system('eog fig5.svg')
# os.system('rm fig5.svg')
#+end_src

#+RESULTS:
: None



     


*** Fitting coeff panel (model+exp)

**** PANEL
#+begin_src python
import sys
sys.path.append('../3d_scan/')
from plots_for_exp import make_four_histogram

fig0, fig1, fig2, fig3 = \
          make_four_histogram(with_models=\
                    ['LIF', 'EIF', 'sfaLIF', 'sbtaLIF', 'iLIF'], FIGSIZE=(8,6),\
                             path = '../3d_scan/')
fig0.savefig('../figures/0.svg', format='svg')
fig1.savefig('../figures/1.svg', format='svg')
fig2.savefig('../figures/2.svg', format='svg')
fig3.savefig('../figures/3.svg', format='svg')

import svgutils.transform as sg
import os
fig = sg.SVGFigure("18cm", "4cm")


# load matpotlib-generated figures
fig1 = sg.fromfile('../figures/0.svg')
fig2 = sg.fromfile('../figures/1.svg')
fig3  = sg.fromfile('../figures/2.svg')
fig4 = sg.fromfile('../figures/3.svg')

# get the plot objects
plot1 = fig1.getroot();plot1.moveto(2, 2, scale=.5)
plot2 = fig2.getroot();plot2.moveto(130, 2, scale=.5)
plot3 = fig3.getroot();plot3.moveto(260, 2, scale=.5)
plot4 = fig4.getroot();plot4.moveto(390, 2, scale=.5)

# add text labels
txt1 = sg.TextElement(0,10, "A", size=14, weight="bold")
txt2 = sg.TextElement(120,10, "B", size=14, weight="bold")
txt3 = sg.TextElement(250,10, "C", size=14, weight="bold")
txt4 = sg.TextElement(380,10, "D", size=14, weight="bold")

# append plots and labels to figure
fig.append([plot1, plot2, plot3, plot4])
fig.append([txt1, txt2, txt3, txt4])

# save generated SVG files
fig.save("fig_3d_coeff.svg")


# os.system('inkscape --export-pdf=fig_3d_coeff.svg fig_3d_coeff.pdf)
os.system('eog fig_3d_coeff.svg')
# os.system('rm fig5.svg')
#+end_src

#+RESULTS:
: None



     

**** export-pdf

#+begin_src python
import os
os.system('inkscape --export-pdf=fig_3d_coeff.pdf fig_3d_coeff.svg')
#os.system('rm fig_3d_coeff.svg')
#+end_src

#+RESULTS:
: None


*** Heterogenity panel (model+exp)

**** PANEL
#+begin_src python
import sys
import svgutils.transform as sg
import os
fig = sg.SVGFigure("10cm", "17cm")

# load matpotlib-generated figures
fig1 = sg.fromfile('../figures/muG_Tv_model_firing.svg')
fig2 = sg.fromfile('../figures/muG_Tv_model_subthre.svg')
fig3  = sg.fromfile('../figures/muG_Tv_data_full_spiking.svg')
fig4 = sg.fromfile('../figures/muG_Tv_data_full_subthre.svg')
fig5 = sg.fromfile('../figures/muG_Tv_data_full_vthre.svg')
fig6 = sg.fromfile('../figures/muG_Tv_data_full_hist.svg')

# get the plot objects
plot1 = fig1.getroot();plot1.moveto(2, 2, scale=.48)
plot2 = fig2.getroot();plot2.moveto(2, 115, scale=.48)
plot3 = fig3.getroot();plot3.moveto(140, 2, scale=.5)
plot4 = fig4.getroot();plot4.moveto(140, 140, scale=.5)
plot5 = fig5.getroot();plot5.moveto(140, 290, scale=.5)
plot6 = fig6.getroot();plot6.moveto(80, 400, scale=.5)

# add text labels
txt1 = sg.TextElement(0,15, "A", size=14, weight="bold")
txt2 = sg.TextElement(0,110, "B", size=14, weight="bold")
txt3 = sg.TextElement(145,15, "C", size=14, weight="bold")
txt4 = sg.TextElement(145,120, "D", size=14, weight="bold")
txt5 = sg.TextElement(145,295, "E", size=14, weight="bold")
txt6 = sg.TextElement(100,395, "F", size=14, weight="bold")

# append plots and labels to figure
fig.append([plot1, plot2, plot3, plot4, plot5, plot6])
fig.append([txt1, txt2, txt3, txt4, txt5, txt6])

# save generated SVG files
fig.save("fig5.svg")


# os.system('inkscape --export-pdf=fig5.pdf fig5.svg')
os.system('eog fig5.svg')
# os.system('rm fig5.svg')
#+end_src

#+RESULTS:
: None



     


* Preamble (options for LaTeX formatting) :noexport:

#+LATEX_CLASS: article
# #+OPTIONS: toc:nil (no Table Of COntents at all)
#+LaTeX_HEADER: \usepackage{geometry}
#+LaTeX_CLASS_OPTIONS: [10pt,a4paper,twoside,colorlinks]
#+LaTeX_HEADER: \geometry{a4paper,total={210mm,297mm}, left=15mm, right=15mm, top=20mm, bottom=20mm, bindingoffset=0mm, columnsep=.5cm}
#+LaTeX_HEADER:\usepackage{graphicx}
#+LaTeX_HEADER:\usepackage[AUTO]{inputenc}
#+LaTeX_HEADER:\usepackage[T1]{fontenc}
#+LaTeX_HEADER:\usepackage{lmodern}
#+LaTeX_HEADER:\usepackage{amssymb,mathenv,array, amsmath}
#+LaTeX_HEADER: \renewcommand{\refname}{\vspace{-.8cm}}
#+LaTeX_HEADER: \usepackage{filecontents}
#+LaTeX_HEADER:\usepackage{microtype} % Slightly tweak font spacing for aesthetics
# #+LaTeX_HEADER: \renewcommand\thesection{\Roman{section}}
# #+LaTeX_HEADER: \titleformat{\section}[block]{\Large\bfseries}{\thesection.}{.5em}{} 
# #+LaTeX_HEADER:\renewcommand{\@fnsymbol}[1]{ \ensuremath{    \ifcase#1  \or *    \or \dagger    \or \ddagger    \or \mathsection    \or \mathparagraph    \else      \@ctrerr    \fi }}
#+LaTeX_HEADER: \hypersetup{allcolors = blue} % to have all the hyperlinks in 1 color

** Headers and footers
#+LaTeX_HEADER: \usepackage{fancyhdr} % Headers and footers
#+LaTeX_HEADER: \pagestyle{fancy} % All pages have headers and footers
#+LaTeX_HEADER: \fancyhead{} % Blank out the default header
#+LaTeX_HEADER: \fancyfoot{} % Blank out the default footer
#+LaTeX_HEADER: \fancyhead[C]{\footnotesize \shorttitle \quad $\bullet$ \quad \shortauthor \quad $\bullet$ \quad \shortdate \normalsize }
#+LaTeX_HEADER: \fancyfoot[C]{\thepage} % Custom footer text
#+LaTeX_HEADER: \makeatletter


** Title and Authors

#+LaTeX_HEADER: \usepackage{titlesec} % Allows customization of titles
## ## WE EXPLICIT THE FOOTNOTEMARK IN THE AUTHORS (for easier change) :
#+LaTeX_HEADER:\renewcommand{\@fnsymbol}[1]{  \ensuremath{    \ifcase#1  \or *    \or \dagger    \or \ddagger    \or \mathsection    \or \mathparagraph    \else      \@ctrerr    \fi  } }

#+LaTeX_HEADER: \def\@maketitle{  \newpage  \null  \vspace{-10mm}   \begin{center}  \let \footnote \thanks    {\LARGE \textbf{\@title} \par}    \vskip 1.2em    {\large      \lineskip .5em      \begin{tabular}[t]{c}        \scshape      \normalsize        \@author      \end{tabular}\par}   \vskip .6em   { \@date}  \end{center}  \par  \vskip 1em}
#+LaTeX_HEADER: \makeatother
#+LaTeX_HEADER: \renewcommand{\thefootnote}{\arabic{footnote}} % we restor the arabic number footnotes


** Short titles/author

#+LaTeX_HEADER: \def\shorttitle{Firing rate response in the \textit{fluctuation driven} regime} 
#+LaTeX_HEADER: \def\shortauthor{Zerlaut et al.} 
#+LaTeX_HEADER: \def\shortdate{\today} 



* Supplementary Information 29/11/2015 				   :noexport:
\beginsupplement

** Details about the experimental data
<<sec:exp-details>>


#+ATTR_LATEX: :placement [b!]
#+NAME: fig:experimental-details
#+CAPTION: *Details about the presented dataset* (animal age,  electrical access and membrane properties). *(A)* Histogram of the access resistance. *(B)* Histogram of the "Seal Quality", the current leak between the pipette and the patch of membrane. *(C)* Histogram of the full recording time. Corresponding either to the loss of cellular access (rarely) or to the exit of the criteria formulated in Section[[sec:monitoring]] (most common case). *(D)* Histogram of the membrane time constants. *(E)* Histogram of the membrane input resistance. *(F)* Histogram of the animal post-natal day per recorded cell. *(G)* Cross correlations (Pearson correrlation) between all monitored quantities.
[[./figures/experimental_details.png]]

 For each cell, its properties and the quality of the electrical
 access was quantified. We present here those data and look for
 relations between them.

Some of the relations that appear are :

- Very naturally, the membrane time constant is proportional to the input resistance
  (c=0.8, $p<2.10^{-10}$)

- The recording time diminishes with the age of the animal 
  ($p<1.10^{-3}$)

- The membrane resistances and membrane time constants decrease with
  the age of the animal even if they keep a strong variability
  ($p<5.10^{-3}$ and $p<3.10^{-2}$ respectively).

- The access resistance seems independent of all parameters

- The quality of the seal does not seem to impede much the duration of
  the recording (despite the fact that they are both correlated with
  the age of the animal, there mutual correlation is low).


\newpage


** Accuracy of the single compartment approximation
<<sec:single-comp>>

#+ATTR_LATEX: :placement [hb!]
#+NAME: fig:single-comp-approx 
#+CAPTION: *Accuracy of the single compartment approximation in the neocortical neurons of our recordings*.*(A)* Histogram of the accuracy coefficient $C_\textrm{sc}$. *(B)* Neuron showing the best accuracy coefficient. *(C)* Neuron showing the worst accuracy coefficient.
[[./figures/single_comp.png]]

The single-compartment approximation is important in this study as it
is used to constrain the fluctuations of the membrane potential. We do
here a cell by cell quantification of the accuracy of the
approximation. 

 We quantify the accuracy of the approximation as follows. We take the
 protocols that were used to determine the membrane properties: prior
 to each protocol, we recorded and averaged the response to 10 current
 pulses of $\sim$ 500ms and of $\Delta I \sim$ 15pA amplitude, not the
 (noisy) continuous monitoring presented in Figure [[fig:exp-charact]]. We
 average over trials the membrane potential response and fit an
 exponential load to this mean response
 $V_\mathrm{sc}^\mathrm{fit}(t)$, we get a membrane time
 $\tau_\mathrm{m}^0$ and a membrane resistance $R_\mathrm{m}^0$. We
 define the residual trace as the normalized absolute difference
 between the fit and the trace :

\begin{equation}
\mathrm{Res}(t) = \frac{ \| V(t) - V_\mathrm{sc}^\mathrm{fit}(t) \| }{R_m^0 \Delta I}
\label{eq:residual-single-comp}
\end{equation}

 
 Now we quantify the accuracy of the single-compartment approximation
 by taking the (normalized) integral over 7 membrane time constant of
 the residual trace (see bottom traces in [[fig:single-comp-approx]]B and
 [[fig:single-comp-approx]]C).

\begin{equation}
C_\mathrm{sc} = \int_{t_0}^{t_0+7 \, \tau_m^0} \frac{dt}{7\,\tau_m^0}
\mathrm{Res}(t)
\label{eq:accuracy-coeff-single-comp}
\end{equation}

 The two normalizations (by membrane resistance and membrane time
 constant) were performed to yield comparable quantities for different
 membrane parameters.

 We present the histogram of this quantity over the
 cells of the dataset in Figure [[fig:single-comp-approx]]A.

 We conclude that the approximation was satisfactory : the worst case
in Figure [[fig:single-comp-approx]]C corresponds to a pretty good
match. 

\newpage

**** analysis 							   :noexport:

#+begin_src python
import sys
sys.path.append('/home/yann/work/python_library/')
from electrophy import IC_membrane_test as IC
sys.path.append('../experimental_data/')
import dataset_structure as DATA
import numpy as np

RESIDUAL_LIST, CELL_LIST = [], []

Tm_factor = 6

for i in DATA.CELL_LIST[np.concatenate([np.arange(28), np.arange(29,len(DATA.CELL_LIST))])]:
   exec('from cell'+str(i)+' import cell'+str(i))
   exec('cell = cell'+str(i)+'.cell_params')
   exp, time, t, data, params = IC.load(cell['ROOT_FOLDER']+cell['IC_datafile'])
   exp, time, t, data, params, Rm, El, Cm, t_fit, v_fit,\
       RmS, CmS, Ra, RmD, CmD, v_fit_2comp, mean_v_response, mean_i = \
                      IC.analyze(exp, time, t, data, params)
   Tm = Rm*Cm*1e-3
   dt = t[1]-t[0]
   DI = np.abs(np.diff(mean_i[5:])).max() # pA, pulse
   # we find where the pulse start !
   i1 = np.where(np.abs(np.diff(mean_i[5:]))>.6*DI)[0]
   it = np.arange(i1[0], min([i1[0]+int(Tm_factor*Tm/dt), len(t_fit)-1]))
   residual = np.abs((mean_v_response[it]-v_fit[it])/(v_fit[-1]-v_fit[0]))
   RESIDUAL_LIST.append(residual.sum()*dt/Tm/Tm_factor)
   CELL_LIST.append(i)

# then the cell28, that has an AP in one trial and can not be evaluated !
CELL_LIST.append(DATA.CELL_LIST[28])
RESIDUAL_LIST.append(np.array(RESIDUAL_LIST).mean())
CELL_LIST = np.array(CELL_LIST)
RESIDUAL_LIST = np.array(RESIDUAL_LIST)
isort = np.argsort(CELL_LIST)
np.save('../experimental_data/analyzed_data/residuals.npy',\
                   [CELL_LIST[isort], RESIDUAL_LIST[isort]])
#+end_src

#+RESULTS:
: None


**** plot 							   :noexport:

#+begin_src python
import sys
sys.path.append('/home/yann/work/python_library/')
from electrophy import IC_membrane_test as IC
from my_graph import set_plot
sys.path.append('../experimental_data/')
import dataset_structure as DATA
import matplotlib.pylab as plt
import numpy as np

CELL_LIST, RESIDUAL_LIST = \
     np.load('../experimental_data/analyzed_data/residuals.npy')


figA = plt.figure(figsize=(4,3))
plt.subplots_adjust(bottom=.25, left=.25)
ax = plt.subplot(111)
plt.hist(RESIDUAL_LIST, bins=15, color='grey')
set_plot(ax, xlabel='$C_{sc}$', ylabel='cell count')

Tm_factor = 10

imax = np.argmax(RESIDUAL_LIST)
imin = np.argmin(RESIDUAL_LIST)

FIG = []
for i in [DATA.CELL_LIST[imin], DATA.CELL_LIST[imax]]:
   f, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(5,5))
   plt.subplots_adjust(bottom=.15, left=.25)
   FIG.append(f)
   exec('from cell'+str(i)+' import cell'+str(i))
   exec('cell = cell'+str(i)+'.cell_params')
   exp, time, t, data, params = IC.load(cell['ROOT_FOLDER']+cell['IC_datafile'])
   exp, time, t, data, params, Rm, El, Cm, t_fit, v_fit,\
       RmS, CmS, Ra, RmD, CmD, v_fit_2comp, mean_v_response, mean_i = \
                      IC.analyze(exp, time, t, data, params)
   Tm = Rm*Cm*1e-3
   dt = t[1]-t[0]
   DI = np.abs(np.diff(mean_i[5:])).max() # pA, pulse
   # we find where the pulse start !
   i1 = np.where(np.abs(np.diff(mean_i[5:]))>.6*DI)[0]
   it = np.arange(i1[0], min([i1[0]+int(Tm_factor*Tm/dt), len(t_fit)-1]))
   residual = np.abs((mean_v_response[it]-v_fit[it])/(v_fit[-1]-v_fit[0]))
   ax1.plot(t_fit, mean_i[:len(t_fit)], 'k')
   set_plot(ax1, ylabel='I (pA)', spines=['left'])
   ax2.plot(t_fit, mean_v_response[:len(t_fit)], 'k')
   ax2.plot(t_fit[it], v_fit[it], 'r--', lw=3)
   set_plot(ax2, ylabel='$V_m$ (mV)', spines=['left'])
   ax3.plot(t_fit[it], residual, 'k-')
   ax3.plot(t_fit, 0*t_fit, 'k-')
   ax3.fill_between(t_fit[it], residual, 0*residual, color='grey')
   ax3.plot([10,10],[0,0.08],color='w')
   set_plot(ax3, ylabel='residual', xlabel='time (ms)')

figA.savefig('../figures/single_comp_accuracy_hist.svg', format='svg', tranparent=True)
FIG[0].suptitle('best match')
FIG[0].savefig('../figures/single_comp_accuracy_best.svg', format='svg', tranparent=True)
FIG[1].suptitle('worst match')
FIG[1].savefig('../figures/single_comp_accuracy_worst.svg', format='svg', tranparent=True)

import svgutils.transform as sg
fig = sg.SVGFigure("12.5cm", "4.5cm")

# load matpotlib-generated figures
fig1 = sg.fromfile('../figures/single_comp_accuracy_hist.svg')
fig2 = sg.fromfile('../figures/single_comp_accuracy_best.svg')
fig3 = sg.fromfile('../figures/single_comp_accuracy_worst.svg')

# get the plot objects
plot1 = fig1.getroot();plot1.moveto(2, 20, scale=.5)
plot2 = fig2.getroot();plot2.moveto(160, 2, scale=.4)
plot3 = fig3.getroot();plot3.moveto(310, 2, scale=.4)

# add text labels
txt1 = sg.TextElement(0,20, "A", size=14, weight="bold")
txt2 = sg.TextElement(155,15, "B", size=14, weight="bold")
txt3 = sg.TextElement(305,15, "C", size=14, weight="bold")

# append plots and labels to figure
fig.append([plot1, plot2, plot3])
fig.append([txt1, txt2, txt3])

# save generated SVG files
fig.save("../figures/single_comp.svg")

import os
os.system('inkscape --export-pdf=../figures/single_comp.png ../figures/single_comp.svg')
os.system('eog ../figures/single_comp.svg')
# os.system('rm fig2.svg')

#+end_src

#+RESULTS:
: None


\newpage


** Controlling the $V_m$ fluctuations properties :noexport:
<<sec:fluctuation-control>>

#+ATTR_LATEX: :placement [hb!]
#+NAME: fig:approx-allows-control
#+CAPTION: *Checking whether we can control accurately the V_m fluctuations*. It works well in the subthreshold range, but active currents (proportional to spiking and/or depolarization) as well a the procedure of removing spikes have a strong impact on the V_m fluctuations.
[[file:./figures/approx_allows_control_of_fluct.png]]


\newpage 


** A linear /phenomenological threshold/ allows an accurate characterization of the firing rate response
<<sec:linear-threshold-is-good>>

todo :

- write the 11 params form

#+NAME: eq:11-params-threshold
\begin{equation}
ksdjfh
\end{equation}

see Figure [[fig:robustness-all-data]]

#+ATTR_LATEX: :placement [hb!]
#+NAME: fig:robustness-all-data
#+CAPTION: *A Linear threshold is almost as good as an 11 parameters fit. We compare different fit with different number of parameters, all being a subform of expression Equation [[eq:11-params-threshold]].* *(A)* For the theoretical models. *(B)* For the data.
[[file:./figures/linear_threshold_is_good.png]]


\newpage 


** Full dataset for the firing rate response

#+ATTR_LATEX: :placement [hb!]
#+NAME: fig:all-data
#+CAPTION: *Firing rate response for all cells (n=25) of the dataset*. On top of each 3d-graph, the passive parameters and the parameters of the fitted linear threshold are presented.
[[file:./figures/all_data.png]]


\newpage


** Robustness of the characterization
<<sec:more-robustness>>

Additional analysis about the robustness of the characterization

#+ATTR_LATEX: :placement [hb!]
#+NAME: fig:robustness-supp
#+CAPTION: *Additional analysis for the robustness of the characterization*. *(A)* Robustness analysis of the characterization when including the n=25 cells*. This means that the minimum size of a slice is n_{points} = 12 (so that we potentially scan a biased sample of the  \((\mu_V, \sigma_V, \tau_V) \) configurations). *(B)* Robustness analysis as in Figure [[fig:data-response-and-fit]]B but instead of looking at the parameters of the /phenomenological threshold/ we look at the properties of the firing rate response itself (as defined in section[[sec:sensitivities]]).
[[file:./figures/robustness_supp.png]]


\newpage 


** Deintricating the mechanism leading to the observed sensitivity to the speed of the fluctuations
<<sec:muG-Tv-deintricate-supp>>

#+NAME: fig:muG-Tv-merged
#+ATTR_LATEX: :placement [hb!]
#+CAPTION: *Deintricating the sensitivity to the speed of the fluctuations*. *(A)* We increase the shunting somatic conductance $\mu_G$ while keeping the fluctuations properties equal. The firing is slightly suppressed but this level of suppression is already the trace of a smooth activation curve (because the effect should be weak). *(B)* As in the previous sections, we comodulate the total conductance $\mu_G$ and the global autocorrelation time $\tau_V$, we obtain the same mean sensitivity to $\tau_V$. *(C)* Now we decrease $\tau_V$ without increasing $\mu_G$, the sensitivity is increased.
[[file:./figures/muG_Tv_merged.png]]


\newpage


** Implications for population rate dynamics :noexport:
<<sec:impact-on-rate-coding>>

 We showed that the firing rate of neocortical neurons exhibits a
 strong dependency on the speed of the membrane potential fluctuations
 under physiological conditions (Section [[muG-Tv]]): close to the IaF
 model although impaired by the conductance dependency. This is a
 priori difficult to implement biologically as it would require an
 infinitely sharp activation curve. But we showed that sodium
 inactivation is able to maintain a strong sensitivity to the speed of
 the fluctuations despite a finite sharpness of the activation
 curve. This would give another /functional/ role to the sodium
 inactivation mechanism ([[cite:Platkiewicz2011]]). 

 This optimization suggests that this feature is important for
 population coding in neocortical networks. Transient variations of
 the speed of the fluctuations across the population seems to be a
 reliable mechanism to change the network firing rate (though
 detectable only at the $\ge$ 5 ms time scale). The encoding power of
 this variable appears to be optimized by the cellular properties.

 On the other hand, our quantification of the dependency on $\mu_V$
 and $\sigma_V$ has put in perspective their encoding power, as we
 observed a strong reduction with respect to the Integrate and Fire
 model. Unlike the optimization that happens for the $\tau_V$
 variables, there does not seems to be a biophysical mechanism that
 enhances the encoding power of $\mu_V$ and $\sigma_V$, on the other
 hand . Nevertheless, even if there are not promoted, they still keep
 a high encoding power as they can lead to stronger variations than 
the normalized autocorrelation time.

 To illustrate this, we return to the situation presented above of a
 2-3 fold conductance increase with respect to background activity
 level (a rather strong stimulation) would correspond to $\sim$ 50 %
 decrease of normalized autocorrelation time and would lead to a
 change of 3.5 Hz of firing rate. According to the mean behavior
 within the fluctuation driven regime, this would be achieved by a
 shift of \(\sim 5\mathrm{mV}\) for $\mu_V$ and of \(\sim
 3.5\mathrm{mV}\) for $\sigma_V$.

 Finally, our measurements showed that a change of conductance can, by
 itself, change the firing rate and therefore encode information
 because of its suppressive effect on the spiking probability. This
 effect remains weak though.



** Relevant models to account for the firing rate response of neocortical neurons at a mesoscopic scale :noexport:
<<response-to-in-vivo-like>>

Reduced neuronal model are widely used in theoretical studies to
understand the computational properties of neural networks, especially
in the asynchronous regime (the analogous at the network level of the
/fluctuation-driven/ at the cellular level). It is therefore an
important question to understand the potential limitations of those
models.

When looking at the response as a function of the two first moments of
the membrane potential fluctuations, we found very similar results to
the one presented in [[cite:Rauch2003]]. The same function (with
adjusted parameters) was able to reproduce the firing rate response
for the IaF model as well as for the neocortical neurons of our
experimental model. Thus suggesting that the IaF model is a sufficient
model to qualitatively describe the [[cite:Rauch2003]] if we limit
ourselves to those two moments.

But when we introduced the effect of the somatic conductance $\mu_G$
and the /global autocorrelation/ time $\tau_V$, we observed important
discrepancies between the response of the IaF neuron and neocortical
neurons. First, the total conductance had no effect on the IaF neuron
whereas it exerts a suppressive effects on the reponse of neocortical
neurons. An even stronger discrepancy was observed in the relation to
the temporal fluctuations, the IaF model has a rather trivial response
(more firing for faster fluctuations), whereas neocortical neurons
show a more complex response because of the inactivation dynamics of
sodium channels or due to subthreshold adaptation.


More importantly, the IaF model over-estimate the contributions of
$\mu_V$ and $\sigma_V$ in the coding of information under rate coding
strategies.


# At hyperpolarized levels, as
# the IaF model, they fire more for fast fluctuations (with a stronger
# sensitivity because inactivation penalizes slow fluctuations) and this
# relation can reverse at depolarized levels.

# The EIF models (for varying $k_a$) also miss this non trivial
# dependency to the fluctuations, whereas they can account for the
# dependency on the input conductance.

# The Wang-Buszáki model showed this dependency but its excitability
# level lies pretty far from the one observed in our recorded pyramidal
# cells, thus making the comparison difficult.



** Link between the spike initiation properties and the input-output properties :noexport:
 <<sec:spk-mech>>

The properties of the spike initiation mechanism should determine the
characteristics of the firing rate response reported in this study.

At the soma, the spike seems to be initiated abruptly
([[cite:Naundorf2006]]), whether this is a functional property to extract
fast signals ([[cite:Naundorf2006, Tchumatchenko2011, Ilin2013]] and
[[cite:Brette2013]] alternatively) or an epiphenomena due to
backpropagation toward the soma [[cite:McCormick2007]] is still debated
(reviewed in [[cite:Brette2015]]).

The measurements and analysis performed here provides some additional
material for theoretical considerations in this controversy.

We showed that neurons

 On the one hand, we show that another /functional/ advantage of
having a sharp activation is to /resist/ to an increasing input
conductance (in the sense that the shunting due to background synaptic
conductances would not impact the efficiency of the sodium current at
eliciting spikes). For example, thanks to his infinitely sharp
activation curve, the LIF model has this characteristics (see Figure
[[]]). Nevertheless, neocortical neurons showed a dependency that
could be explained only by a smooth activation curve ($k_a \sim 3-4
\mathrm{mV}$) in a single compartment model.

 On the other hand, 
 Because we investigated only single-compartment models, we suggested
that this was the consequence of the finite sharpness (Section [[muG]])
but an increasing the somatic input conductance also affects the
electrical proximity between the soma and the initiation site (despite
the constant electrotonic distance) thus potentially filtering the
membrane potential fluctuations. Interpreting this measurement in the
light of the existing models ([[cite:Naundorf2006]] or [[cite:McCormick2007]]
or [[cite:Brette2013]]) should be the focus of future investigations as it
could be a key additional constraint for theoretical models.

 # Additionally, the remarkable sensitivity to the speed of the
 # fluctuations (Figure [[fig:Tv][3]]C) requires that fast fluctuations are actually
 # extracted, therefore confirming several measurements performed in
 # /current-clamp/ [[cite:Tchumatchenko2011]], [[cite:Ilin2013]] (the
 # inactivation mechanism increases the dependency on $\tau_V$ but is
 # not able to create a strong dependency by itself).

#  Matching the combination of those two properties with a sharp somatic
#  spike ($k_a \sim 1-2 \mathrm{mV}$ in our recordings from the dynamic
#  I-V curve analysis [[cite:Badel2008]], not shown) 
# could be a key additional constraint for theoretical models.



** Heterogeneity within the neocortical pyramidal cell population :noexport:
<<sec:discussion-heterogeneity>>

Our approach provided a way to quantify heterogeneity of the firing
rate response in pyramidal neurons by looking at the dispersion


of
the $\Delta V_\mathrm{thre}$ coefficients (after substraction of
the expected Poisson dispersion). We show a sum up of those
properties on [[X]].

   First pyramidal cells seems to have a very strong variability of
   excitability levels ($\sigma (P_0) \sim 7
   \mathrm{mV}$) and also seem to be differently affected by sodium
   inactivation (see [[fig:3D][Figure 5 E & F]]).
   
 The question of the functional advantage of cellular has been tackled
 in a theoretical study [[cite:Mejias2012]], the authors found that there
 might be an optimal heterogeneity level for neural coding in
 spiking networks. 

 Importantly, the cellular heterogeneity considered in their study
 only covers the /mean excitability/ heterogeneity in our data, the
 impact on the computation of spiking networks of the heterogeneity of
 the three other components remains to be investigated and would be
 the focus of future analysis.

 Neocortical neurons in this experimental model are not mature
 neurons, a possibility is that the measured heterogeneity is the
 consequence of differences in the maturation level of individual
 cells. 


** Higher order description of the dynamical state at the soma :noexport:
<<higher-order>>

Although we believe that the four variables ($\mu_V$, $\sigma_V$,
$\mu_G$, $\tau_V$) described in this study might quantitatively be the
main contributor to the firing rate response, they constitute a very
reductive description of the dynamical state at the soma. 

First, the stationary distribution of the subthreshold fluctuations
could deviate from the gaussian approximation (*REF?*), potentially
because of the assymetry of the driving force between excitation and
inhibition (unlike the symmetry considered in this study). For the
temporal dynamics, we provided a very approximated quantity: the
\textit{global autocorrelation time} $\tau_V$, but it is very likely
that the details of the autocorrelation function shape have a strong
impact on the firing probability (because of the specificity of the
time and voltage dependency of sodium channels). 

# We added the somatic input conductance as an additional variable,
# again it is likely that an accurrate description would need to include
# other somatic variables such as sodium inactivation level or potassium
# activation levels [[cite:Platkiewicz2011]].

Finally, we have put all non-linear effects (subthreshold adaptation,
sodium inactivation, spike frequency adaptation) in the /black box/ of
the firing rate response. A more careful analysis could take into
account the impact of those properties on the membrane potential
fluctuations and then investigate spiking probability.



** Treatment of adaptation :noexport:
<<adaptation>>

No treatment of the 
 

`** /De/-normalization of the response :noexport:
<<denormalization>>

For all neurons, despite their different electrophysiological
properties, we investigated a fixed domain of the two normalized
quantities $\frac{\tau_V}{\tau_\mathrm{m}^0}$ and
$\frac{\mu_G}{g_\mathrm{L}}$ and of the membrane potential quantities
$\mu_V$, $\sigma_V$, (where the stimulation needs to be adapted as a
function of the membrane parameters). This procedure allowed a
cell-by-cell comparison but this is now a question how dendritic
integration \textit{de-normalizes} the response by bringing the neuron
in a specific subspace of the ($\mu_V$, $\sigma_V$, $\tau_V$, $\mu_G$)
space for the same presynaptic firing frequencies.

For example, under the hypothesis that the synaptic weights and time
constant are fixed across cells, a small cell (\sim low $g_\mathrm{L}$
and $C_\mathrm{m}$) will explore a bigger $\sigma_V$ space at low
$\mu_G$ than a big cell because synaptic events will imply larger
variations from the mean. This kind of effects will be another
contributor to the cell to cell variability, this will be investigated
in further studies.



** Application to non stationary dynamics :noexport:
<<non-stationary-case>>

The results above apply to the stationary firing rate. They are
necessary to compute the stationary properties of the dynamics of a
recurrent network, but what is the relevance of those results for the
dynamics in response to time-varying stimuli ?

A first deviation, might be an overestimation of the adaptation effect
(so an underestimation of the firing rate response) because the
population response will involve a few spikes at the cellular level
while the firing rate response for those higher levels have been
evaluated while adaptation was already at its stationary level. But if
the firing rate response of the population remains low, the error
induced by our estimate might remain low.

% For the more general non stationary effects occuring at the population
% level in a network stimulated by transient stimuli, we should mention
% here that we tested the use of this stationary rate estimate in a mean
% field model ([[cite:ElBoustani2009]]) estimate Zerlaut \& Destexhe,
% unpublished observations.

# % A generalization of our approach, including a more precise treatment
# % of time-varying stimuli seems possible. For example, a combination
# % between our heuristic approach and the time-varying threshold equation
# % proposed by [[cite:Platkiewicz2010]] could give interesting results.



** Impact on network dynamical properties :noexport:
<<network-applic>>

A natural extension of this work is to investigate how this firing
rate estimate can be use as the cellular transfer functions in
macroscopic models of network dynamics.\\

Preliminary results (Goethals 2014) showed that this template was able
to account for the stationary as well as the transient dynamics of
artificial neural networks displaying asynchronous activity
([[cite:Kumar2008]]).

The investigation of the consequences of those results to the
dynamical properties of neural networks will be investigated in future
studies. 

Very briefly, the decreasing relationship as a function of the total
conductance observed in neocortical neurons could help to stabilize
the point of self-sustained activity in recurrent networks
([[cite:Kuhn2004]], [[cite:Kumar2008]]).

In [[cite:Destexhe2009a]], it was shown that including different
electrophysiological types of neurons in recurrent networks could
either stabilize asynchronous activity or trigger slow population
oscillations depending on the parameters. In the present study, we
provided a simple analytical form, quite easy to manipulate
analytically, where variations of parameter's template account for
those different elctrophysiological classes. It is now achievable to
study analytically the impact (at least to first order) of those
complex biophysical features on network dynamics. This is currently
under investigation.


# # % \subsection{Non gaussian membrane potential distributions}
# # % \label{sec:non-gaussian}

# # % Though the \textit{diffusion approximation} might hold \textit{in
# # %   vivo} (\todo{refs}), many studies are performed on small networks
# # % with sparse connectivity and high weights (in particular networks of
# # % IaF neurons with conductance-based synapses ) where we are pretty far
# # % from the \textit{diffusion approximation} domain. Those studies are
# # % important to understand the computational properties of recurrent
# # % networks. However they do not benefit yet of good estimates for the
# # % cellular transfer function to perform mean-field analysis. We did not
# # % base our transfer function template on results based first passage
# # % time of stochastic processes but we choose a more abstract template at
# # % the cost of having to do some fitting. We show in our companion paper
# # % (Zerlaut et al. XXXX) that the flexibility introduced by our template
# # % is able to capture this non-gaussianity and works also far from the
# # % \textit{diffusion approximation} domain.

# # % \subsubsection{Impact of correlation in the 
# # %   presynaptic spike trains}
# # % \label{sec:correlation}

# # % Because \textit{mean-field} theories usually requires negligible
# # % cell-to-cell correlations, we hypothesized that the presynaptic spike
# # % trains are generated from $K$ independent Poisson processes.
# # % Including correlations between the $K$ processes would not be a major
# # % issue in our framework. If our hypothesis that the four variables
# # % determines the firing uniquely holds, correlations only affects the
# # % $\sigma_V$ variable. In our additional communication, we adapt
# # % Campbell's theorem to include correlations between spike trains
# # % (Zerlaut et al., in preparation) and show that this procedure
# # % gives accurate? predictions (to be done, but will be quick!).




** Deintricating the mechanism leading to the observed sensitivity to the speed of the fluctuations :noexport:
<<sec:muG-Tv-deintricate>>

 A striking feature in the firing rate response of neocortical neurons
 is that they can show a remarkable sensitivity to the speed of the
 membrane potential fluctuations.  Our modeling results suggested (see
 Figure [[fig:mean-sensitivities]]C and [[fig:biophysics-explain-heter]]E)
 that a way to obtain this strong sensitivity is to have a mechanism
 that penalizes how slow fluctuations are converted into spikes and
 fast sodium inactivation would be such a mechanism.

 Another possibility for having a strong dependency to the speed of
 the fluctuations is that those neocortical neurons could have a very
 sharp activation curve that would enable them to extract very fast
 input [[cite:Fourcaud-Trocme2003]], such as suggested for more mature
 pyramidal neurons in rat neocortex [[cite:Ilin2013]]. Nevertheless, even
 at the soma, the neurons of our recordings show a rather smooth
 activation curve (k_a \sim 1.5 mV, from the dynamic I-V curve
 analysis [[cite:Badel2008]], not shown) rendering this possibility
 unlikely. On the other hand, our hypothesis is then that, despite
 their relatively smooth activation curve, the sensitivity to the
 speed of the fluctuations is obtained from fast sodium inactivation
 that penalizes slow fluctuations.

 We examine this hypothesis in details in section
[[sec:muG-Tv-deintricate-supp]] and sum it up here. We first evidenced
the relative smoothness of the sodium activation curve by its effect
on the firing probability in the fluctuation-driven regime. Indeed, if
the sodium activation curve is smooth, then an increase of somatic
conductance would have the ability to shunt the sodium current and the
spiking probability should decrease when all fluctuations parameters
are kept identical (\mu_V, \sigma_V, \tau_V^N). We performed those
experiments and indeed observed a decrease of the firing probability
(see Figure [[fig:muG-Tv-merged]]A) corresponding to an activation curve
of k_a \sim 3 mV. This phenomena then gives us a way to test our
hypothesis. In our protocols, to reproduce /in vivo/ conditions, the
fast fluctuations were achieved by increasing the somatic conductance
(to reduce the membrane time constant, see
methods[[sec:stimulation-design]]) meaning that our data contains the
effect of the shunting conductance. Therefore if we achieved fast
fluctuations without the shunting effect (by playing with the
correlations of the input current, see methods[[sec:stimulation-design]])
we should observe an even stronger sensitivity to the speed of the
fluctuations (see Figure [[fig:muG-Tv-merged]]B & C). Indeed, we observed
an increase in the sensitivity to \tau_V under those conditions, the
mean sensitivity over cells was then much higher than the LIF model
(that is a higher bound for the sensitivity in absence of other
mechanism), meaning that fast extraction capabilities contribute few
to the global sensitivity. Taken together, those experiments
demonstrates that the sensitivity comes from a mechanism that
penalizes slow fluctuations. Additionally, the phenomena described
here is the continuous analogous of the phenomena described in
[[cite:Fernandez2011]] for CA1 pyramidal cells, where the authors found
that a high conductance state (corresponding to fast fluctuations in
our study) could evoke more spikes than a low conductance state (slow
fluctuations here). Their study also provides evidence for the role of
fast sodium inactivation in the sensitivity to the speed of the
fluctuations and is therefore compatible with our modeling results.


